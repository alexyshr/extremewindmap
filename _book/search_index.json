[
["index.html", "Introduction 0.1 Background 0.2 Research Aim and Objectives 0.3 Research Question 0.4 Thesis Document Structure", " Introduction This research aims to create non-hurricane non-tornadic maps of extreme wind speeds for three specific recurrence intervals (700, 1700, and 3000 years) covering the Colombian territory. These maps will be combined with existing hurricane wind speed studies, to be used as input loads due to wind for infrastructure design. For each station with wind speeds time histories in the input data, extreme wind speed corresponding to each recurrence interval are calculated using a Peaks Over Threshold onwards POT extreme value model, then wind velocities with the same recurrence interval are spatially interpolated to generate continuous maps for the whole study area. A wind speed linked to a mean recurrence interval - MRI of N-years (N-years return value or return period) is interpreted as the highest probable wind speed along the period of N-years. The annual probability of equal or exceed that wind speed is 1/N. The annual exceedance probability for all velocity values in 700-years output map will be 1/700, for the 1700-years map will be 1/1700, and 1/3000 for the 3000-years final map. There are different methods to model extreme value data, among them are a) sample maxima using a Generalized Extreme Value Distribution onwards GEVD (traditional method), b) POT using a Generalized Pareto Distribution onwards GPD, c) POT using a two-dimensional Poisson Process, that can be homogeneous, non-homogeneous, stationary, and non-stationary (originally know as Point Process approach), and d) POT Poisson-GPD. Following (???) in this research a POT using a non-homogeneous non-stationary two-dimensional Poisson proces was selected, despite there is no R package available to apply this approach. 0.1 Background To design one structure, the horizontal forces wind and earthquake play an starring role. For the study area, Colombia, initially the wind force was considered with the decree 1984 as a fixed velocity \\(100 \\frac{Km}{h}\\), later a continuous map with a return period of 50 years was included in the official design standard of the time (NSR-98), then, with the update to NSR-10, an additional map with return period of 700 yeas was included. In the context of this study, extreme wind analysis is concerned with statistical methods applied to very high values of wind as random variable in a stochastic process, to allow statistical inference from historical data, namely, assess from the ordered sample of wind velocities, the probability of wind events that are more extreme than the ones previously observed and included in the mentioned input sample. Classical reference in this matter is (???), where a detailed study is done about classical extreme value theory and models and threshold models. There are four main approaches to deal with extreme value analysis: - sample maxima associated to a Generalized Extreme Value Distribution - GEV, - exceedances over threshold associated to a Generalized Pareto Distribution - GPD, - the Poisson-GPD, an homogeneous Poisson process for the number of exceedances and a GPD for the excess values, and the exceedances over threshold associated to a non- homogeneous bi-dimensional Poisson process, a Point process approach also known as Peaks Over Threshold - POT - Poisson process. Main details will be discussed here for each method, but as the last one is recommended in Asce2017, a more indeed explanation will be provided in for POT-Poisson Process. 0.1.1 Sample maxima To work with random variables of sample maximum values, the used probability distribution function pdf is the GEV \\[ H(y) = \\exp\\left\\{-\\left(1+\\xi\\frac{y-\\mu}{\\psi}\\right)_+^{-\\frac{1}{\\xi}}\\right\\}, \\] \\((y+=max(y,0))\\) where \\(\\mu\\) is the location parameter, \\(\\psi &gt; 0\\) is a scale parameter, and \\(\\xi\\) is a shape parameter. GEV can be seen as the integration in the same psf of the Gumbel distribution (limit \\(\\xi\\rightarrow0\\)), Fréchet distribution (\\(\\xi&gt;0\\)), and Weibull distribution (\\(\\xi&lt;0\\)). 0.1.2 Exceedances over threshold If the researcher needs to work only with extreme values above an specific threshold, (???) showed that the GEV has a GPD approximation where shape \\(\\xi\\) parameter in previous equation is the same parameter for next equation for GPD, \\[ G(y,\\sigma, \\xi) = 1-\\left(1+\\xi\\frac{y}{\\sigma}\\right)_+^{-\\frac{1}{\\xi}}, \\] ### Poisson-GPD If a rescale of the variable indexes above the threshold is performed, then the exceedances over threshold approach can be seen as a point process, namely, an homogeneous Poisson Process where: The number of exceedances above the threshold has a Poisson distribution with mean \\(\\lambda\\) The excess values follow a GPD with \\(N\\leq1\\) Its cumulative distribution function cdf is \\[ F(y) = \\exp\\left\\{-\\lambda\\left(1+\\xi\\frac{y-\\mu}{\\sigma}\\right)_+^{-\\frac{1}{\\xi}}\\right\\}, \\] 0.2 Research Aim and Objectives Main aim of this research is the estimation of wind extreme velocities to be used as input loads for the design of different types of structures, considering its risk categories, and covering any place in the whole study area. Specific objectives are: Analyze and compare three different sources of historical wind time series, to select and use the best data source (or combination or sources) for research, based on objective criteria, for instance similitude, completeness, coverage, etcetera, to achieve this way a formal support for the decision made in this regard. Select and apply an suitable extreme value analysis method that allows to fulfill wind load requirements defined for the respective authority in the study area Estimate extreme wind values for the stations in the selected input data source, for three MRI (700, 1700, 3000 years), considering non-hurricane studies. Generate continuous maps for MRIs 700, 1700, and 3000 years, using the most suitable spatial interpolation technique, considering the specific characteristics of the input data and advantages and disadvantages of the selected methods Combine output maps from non-hurricane analysis, with existing hurricane studies to allow the inclusion of the research study in the NSR-10 norm. 0.3 Research Question Main question of this research is directed to calculate future extreme velocities (return levels) for infrastructure design, then the research question could be What extreme velocities (return levels) need to be used as load design forces for structures of different use category, in the study area? If we remember that, for the case study area (Colombia), there are predefined requirements or mean return intervals - MRI to design structures depending of it use category, and that this MRI values are 700, 1700, and 3000 years, the research question could be more specific. What extreme velocities (return levels) will be equaled or exceeded with a probability equal to \\(\\frac{1}{MRI}\\) in a given year? What extreme velocities (return levels) will be equaled or exceeded only one time in the period defined for this specific MRIs: 700, 1700, and 3000 years? If we consider not only the annual exceedance probability \\(\\frac{1}{MRI}\\), but also the exposure time (compound probability), understood as the time the structure will be in use, then the question will be What extreme velocities (return levels) will have a occurrence compound probability of 67%, when the exposure time of the structure will be equal to the main return intervals 700, 1700, and 3000 years? 0.4 Thesis Document Structure "],
["1-rmd-data.html", "Chapter 1 Data 1.1 IDEAM 1.2 ISD 1.3 ERA5 1.4 Data Download and Organization 1.5 Data Standardization", " Chapter 1 Data Input data is made up of three different sources a) IDEAM - Institute of Hydrology, Meteorology and Environmental Studies of Colombia http://www.ideam.gov.co, b) ISD - Integrated Surface Database https://www.ncdc.noaa.gov/isd, and c) ERA5 climate reanalysis https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5. Table 1.1: Datasets description Institution Dataset Details IDEAM Historical records at weather stations IDEAM is responsible for the instalation, maintenance and management of all kind of weather stations located everywhere along the country NOAA ISD ISD (Integrated Surface Database. NOAA’s National Centers for Environmental Information - NCEI) Lite: A subset from the full ISD dataset containing eight common surface parameters in a fixed-width format free of duplicate values, sub-hourly data, and complicated flags. ECMWF ERA5 ERA5 is a reanalysis dataset with hourly estimates of atmospheric variables with horizontal resolution of 0.25º (33 kilómeters), this is equally spaced cells every 0.25 degrees Table 1.2: Datasets variables Dataset Variables Description IDEAM vvmx_aut_60 Hourly wind maximun velocity ISD wind speed rate Maximun hourly wind velocity. The rate of horizontal travel of air past a fixed point. ERA5 fg10 10 metre wind gust since previous post-processing fsr Forecast Surface Roughness Table 1.3: Variables units and time Variable Units Time Stations vvmx_aut_60 meters per second Variable from 2001 until today. Irregular time series. 203 Wind speed meters per second Variable from 1941 until today. Note: There is too much variability in time (start, end, and time range) for each station. Irregutal time series. 101 fg10 meters per second 1979-Today 3381 fsr meters per second 1979-Today 3381 Ideal data source to create extreme wind speeds maps should be field observed data from IDEAM, but there are not enough number of stations around the study area to represent all the local wind variability in a huge country with multiple variety of climates and and changing thermal floors, but there are other important motivation to include different sources trying to improve output results: As just mentioned, low quantity of IDEAM stations There are uncertainties related to the way IDEAM anemometers are registering data, then comparison with other data sources are needed to be able to do appropriate data standardization, needed as a prerequisite to the analysis. There is no time continuity in the registration of IDEAM data. Historical time series are different and variable in each station. Importance of ISD database for this study is based on the fact that post-processed ISD database has wind extreme values, and it was used to create extreme wind maps for United States. ISD allows comparison with IDEAM records to take better decisions in order to do needed data standardization. Despite that ERA5 data are not observed data, but forecast, its main advantage is data availability to assess the local climatic variance every 33 square kilometers. 1.1 IDEAM Historical observed wind speeds from 203 in Colombia are managed by the official environmental authority IDEAM. Table 1.4 shows a sample of five IDEAM stations. Figure 1.1 shows a map of IDEAM stations. Table 1.4: IDEAM Stations sample Name[Code] Latitud Longitud EMAS - AUT [26155230] 5.09 -75.51 SAN BENITO - AUT [25025380] 9.16 -75.04 AEROPUERTO ALFONSO LOPEZ - [28025502] 10.44 -73.25 TIBAITATA - AUT [21206990] 4.69 -74.21 ELDORADO CATAM - AUT [21205791] 4.71 -74.15 Figure 1.1: IDEAM Stations Following, the time serie, autocorrelation function, and partial autocorrelation function, for IDEAM station “21205791” will be displayed. Figure 1.2: IDEAM Station - Time Serie Figure 1.3: IDEAM Station ACF Figure 1.4: IDEAM Station PACF 1.2 ISD ISD is a database with environmental variables among then extreme wind speeds. ISD has data for the whole planet, and is based on observed data at meteorological stations in each country, which means that for Colombia is based on IDEAM data. Main advantage is data availability at neighbor countries and specialized post-processing made by NOAA’s National Centers for Environmental Information - NCEI in United States, which facilitates its use.Table 1.5 shows a sample of five ISD stations. Figure 1.5 shows a map of ISD stations. Table 1.5: ISD Stations sample Code Name Latitud Longitud 804400 BARINAS 8.62 -70.22 800810 ALTO CURICHE 7.05 -76.35 801000 BAHIA SOLANO / JOSE MUTIS 6.18 -77.40 802590 ALFONSO BONILLA ARAGON INTL 3.54 -76.38 803150 BENITO SALAS 2.95 -75.29 Figure 1.5: ISD Stations Following, the time series, autocorrelation function, and partial autocorrelation function, for ISD station “802590” will be displayed. select &quot;mydatetime&quot;, &quot;802590&quot; as &quot;X802590&quot; from isd_lite_unstack where &quot;802590&quot; IS NOT NULL Figure 1.6: ISD Station - Time Series Figure 1.7: ISD Station ACF Figure 1.8: IDEAM Station PACF 1.3 ERA5 ERA5 is forecast reanalysis data processed by the European Center for Medium-Range Weather Forecasts - ECMWF with wind speeds time series in square cells matrix of pixels of 0.25 degrees (33 km) covering the whole planet. For the study area was extracted a raster of 69 rows by 49 XXX columns in format NetCDF. Figure 1.9 shows a map of ERA5 stations (cells centers). Figure 1.9: ERA5 Stations (cells centers) 1.4 Data Download and Organization 1.5 Data Standardization Analysis of extreme wind speeds requires data standardization as initial step. All input data must be standardized to represent three important conditions: a) anemometer height of 10 meters, b) open space roughness, and c) averaging time of 3-seconds wind gust. Data for analysis must represent 3-s peak wind speeds 10 meters height above the surface, in open terrain. * 10 meters anemometer height * Open space terrain roughness * 3-s gust averaging time "],
["2-rmd-thefra.html", "Chapter 2 Theoretical Framework 2.1 Probability Concepts 2.2 Statistical Concepts For Extreme Analysis 2.3 Extreme Value Analysis Overview 2.4 Peaks Over Threshold - Poisson Process 2.5 Wind Loads Requirements", " Chapter 2 Theoretical Framework 2.1 Probability Concepts Poisson process is an stochastic method that relies in the concepts of probability distributions. The main functions related to probability for extreme value analysis will be described below. 2.1.1 Probability Density Function - pdf Pdf defines the probability that a continuous variable falls between two points, this is, in pdf the probability is related to the area below the curve (integral) between two points, as for continuous probability distributions the probability at a single point is zero. The term density is directly related to the probability of a portion of the curve, if the density function has high values the probability will be greater in comparison with the same portion of curve for low values. \\[ \\int_a^b f(x)dx = Pr[a \\leq X \\leq b] \\] Equation (2.1) is the Gumbel pdf. \\[\\begin{equation} \\mathrm{ f(x)=\\frac{1}{\\beta} \\exp\\left\\{ -\\frac{x-\\mu}{\\beta} \\right\\} \\exp\\left\\{ -\\exp\\left\\{ -\\left( \\frac{x-\\mu}{\\beta} \\right) \\right\\} \\right\\}, \\quad -\\infty &lt; x &lt; \\infty } \\tag{2.1} \\end{equation}\\] where \\(\\exp\\left\\{.\\right\\} \\mapsto \\mathrm{e}^{\\left\\{.\\right\\}}\\), \\(\\beta\\) is the scale parameter, and \\(\\mu\\) is the location parameter. Location (\\(\\mu\\)) has the effect to shift the pdf to left or right along ‘x’ axis, thus, if location value is changed the effect is a movement of pdf to the left (small value for location), or to the right (big value for location). Scale has the effect to stretch (\\(\\beta &gt; 1\\)) of compress (\\(0 &lt; \\beta&lt; 1\\)) the pdf, if scale parameter is close to zero the pdf approaches a spike. Figure 2.1 shows pdf with location (\\(\\mu\\)) = 100 and scale (\\(\\beta\\)) = 40, using equation (2.1). location = 100 scale = 40 .x &lt;- seq(0, 300, length.out=1000) pdfG &lt;- function(x) { 1/location *exp(-(x-location)/scale)*exp(-exp(-(x-location)/scale)) } .y = pdfG(.x) plot(.x, .y, col=&quot;green&quot;, lty=4, xlab=&quot;Velocities Km/h&quot;, ylab=&quot;Density Function - Gumbel Distribution&quot;, main=paste(&quot;Gumbel - Density Function Gumbel Distribution\\n&quot;, &quot;Location=&quot;, round(location,2), &quot; Scale=&quot;, round(scale,2)), type=&quot;l&quot;, cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6) Figure 2.1: Gumbel pdf Figure 2.2 shows pdf with location (\\(\\mu\\)) = 100 and scale (\\(\\beta\\)) = 40, using function dgumbel of the package RcmdrMisc location = 100 scale = 40 .x &lt;- seq(0, 300, length.out=1000) dfG = dgumbel(.x, location=location, scale=scale) plot(.x, dfG, col=&quot;red&quot;, lty=4, xlab=&quot;Velocities Km/h&quot;, ylab=&quot;Density Function - Gumbel Distribution&quot;, main=paste(&quot;Gumbel - Density Function Gumbel Distribution\\n&quot;, &quot;Location=&quot;, round(location,2), &quot; Scale=&quot;, round(scale,2)), type=&quot;l&quot;, cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6) Figure 2.2: Gumbel pdf - dgumbel function 2.1.2 Cumulative Distribution Function - cdf Cdf is the probability of taking a value less than or equal to x. That is \\[ F(x) = Pr[X &lt; x] = \\alpha \\] For a continuous variable, cdf can be expressed as the integral of its pdf. \\[ F(x) = \\int_{-\\infty}^x f(x)dx \\] Equation (2.2) is the Gumbel cdf. \\[\\begin{equation} \\mathrm{ F(x) = \\exp\\left\\{-\\exp\\left[-\\left(\\frac{x-\\mu}{\\beta}\\right)\\right]\\right\\}, \\quad -\\infty &lt; x &lt; \\infty } \\tag{2.2} \\end{equation}\\] Figure 2.3 shows Gumbel cdf with location (\\(\\mu\\)) = 100 and scale (\\(\\beta\\)) = 40, using equation (2.2). As previously done with pdf, similar result can be achieved using function pgumbel of package RcmdrMisc. location = 100 scale = 40 .x &lt;- seq(0, 300, length.out=1000) cdfG &lt;- function(x) { exp(-exp(-(x-location)/scale)) } .y = cdfG(.x) plot(.x, .y, col=&quot;green&quot;, lty=4, xlab=&quot;Velocities Km/h&quot;, ylab=&quot;Probability&quot;, main=paste(&quot;Gumbel - Cumulative Distribution Function\\n&quot;, &quot;Location=&quot;, round(location,2), &quot; Scale=&quot;, round(scale,2)), type=&quot;l&quot;, cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6) Figure 2.3: Gumbel cdf 2.1.3 Percent Point Function - ppf Ppf is the inverse of cdf, also called the quantile function. This is, from a specific probability get the corresponding value x of the variable. \\[ x = G(\\alpha) = G(F(x)) \\] Equation (2.3) is the Gumbel ppf. \\[\\begin{equation} \\mathrm{ G(\\alpha) = \\mu-\\beta ln(-ln(\\alpha)) \\quad 0 &lt; \\alpha &lt; 1 } \\tag{2.3} \\end{equation}\\] Figure 2.4 shows Gumbel ppf, using equation (2.3). Similar result can be achieved using function qgumbel of package RcmdrMisc. location = 100 scale = 40 .x &lt;- seq(0, 1, length.out=1000) ppfG &lt;- function(x) { location - (scale*log(-log(x))) } .y = ppfG(.x) plot(.x, .y, col=&quot;green&quot;, lty=4, ylab=&quot;Velocities Km/h&quot;, xlab=&quot;Probability&quot;, main=paste(&quot;Gumbel - Percent Point Function\\n&quot;, &quot;Location=&quot;, round(location,2), &quot; Scale=&quot;, round(scale,2)), type=&quot;l&quot;, cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6) Figure 2.4: Gumbel cdf 2.1.4 Hazard Function - hf Using \\(S(x) = 1 - F(x)\\) as survival function -sf, the probability that a variable takes a value greater than x \\(S(x) = Pr[X &gt; x] = 1 - F(x)\\), the hf is the ratio between pdf and sf. \\[ h(x) = \\frac{f(x)}{S(x)} = \\frac{f(x)}{1-F(x)} \\] Equation (2.4) is the Gumbel ppf. \\[\\begin{equation} \\mathrm{ h(x)= \\frac{1}{\\beta}\\frac{\\exp(-(x-\\mu)/\\beta)}{\\exp(\\exp(-(x-\\mu)/\\beta))-1} } \\tag{2.4} \\end{equation}\\] Figure 2.5 shows Gumbel hf, using equation (2.4). location = 100 scale = 40 .x &lt;- seq(0, 3000, length.out=1000) hfG &lt;- function(x) { (1/scale)*(exp(-(x-location)/scale))/(exp(exp(-(x-location)/scale))-1) } .y = hfG(.x) plot(.x, .y, col=&quot;green&quot;, lty=4, xlab=&quot;Velocities Km/h&quot;, ylab=&quot;Hazard&quot;, main=paste(&quot;Gumbel - Hazard Function\\n&quot;, &quot;Location=&quot;, round(location,2), &quot; Scale=&quot;, round(scale,2)), type=&quot;l&quot;, cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6, xilm=c(0,1500)) Figure 2.5: Gumbel cdf #library(reliaR) #plot(.x, hgumbel(.x, mu=location, sigma=scale)) #plot(.x, hra.gumbel(.x, mu=location, sigma=scale)) 2.2 Statistical Concepts For Extreme Analysis In order to approach the extreme value analysis, some statistical concepts are needed to understand the theoretical framework behind this knowledge area. In this section will be introduced the concepts annual exceedance probability, mean recurrence interval - MRI, exposure time, and compound probability for any given exposure time and MRI. As an hypothetical example, a simulated database of extreme wind speed will be used. This database is supposed to have 10.000 years of simulated wind speeds. 2.2.1 Annual Exceedance Probability - \\(P_e\\) Using the previously described database, a question arises to calculate the probability to exceed the highest probable loss due to the simulated winds. It is possible to conclude that there is only one event grater or equal (in this case equal) to the highest probable causing loss in 10.000 years, and it is the highest wind. If we sort the database by wind magnitude in descending order (small winds last), the question is solved calculating the annual exceedance probability Pe with next formula Figure 2.6: Sorted Winds by Magnitude - wind simulation database \\[ P_e = \\frac{Event\\,index\\,after\\,descending\\,sorting}{Years\\,of\\,simulations } = \\frac{1}{10.000}=0.001=0.01\\% \\] because the highest wind will be the first in the sorted list. Same exercise can be done with all winds to construct the annual exceedance probability curve, that in this case will represent the probability to equal or exceed different probable losses due to wind. 2.2.2 Return Period - Mean Recurrence Interval - MRI Continuing with the previous section, if the inverse of the exceedance probability is taken, the return period (in years) is obtained. The return period or Mean Recurrence Interval - MRI is associated with an specific return level (wind extreme velocity). MRI is the numbers of years (N) needed to obtain 63% of change that the corresponding return level will occur at least one time in that period. The return level is expected to be exceeded on average once every N-years. The annual exceedance probability of the return level corresponding to N-years of MRI, is \\(P_e=\\frac{1}{MRI}=\\frac{1}{N}\\). For an specific wind extreme event A, the probability that the event will occur in a period equal to MRI years is 63%. If we analyze for the same period a strongest wind extreme event B, its occurrence probability will be lest than 67%. If the purpose of this research is to design infrastructure considering wind loads, the structure will be more resistant to wind if we design with stronger winds, this is high MRIs, and low annual exceedance probability. Common approach for infrastructure design, considering any type of load (earthquake, wind, etc) is to choose high MRI according to the importance/use/risk/type of the structure. For highly important structures, like hospitals or coliseums, where the risk of collapse must be diminished, the MRI used to design is higher in comparison to common structures (for instance a normal house), which implies less risks for its use and importance. \\[ \\mathrm{ P_e = \\begin{cases} \\begin{split} &amp;1-\\exp\\left(-\\frac{1}{MRI}\\right),\\;for\\,MRI\\,&lt;\\,10\\,years \\\\ &amp;\\frac{1}{MRI},\\;for\\,MRI\\,\\geq\\,10\\,years \\end{split} \\end{cases} } \\] 2.2.3 Compound Exceedance Probability - Pn If time of exposure is consider, understood as time the structure will be in use, it is possible to have a compound probability \\(P_n\\), where \\(n\\) is the exposure period. \\(P_n\\) is the probability that the extreme wind speed will be equaled or exceeded at least one time in \\(n\\) years, and is related with the occurrence probability, but also is possible to calculate the non-occurrence compound probability (probability that the event will not occur). \\[ \\mathrm{ P_n = \\begin{cases} \\begin{split} &amp;1-\\left(1-\\frac{1}{MRI}\\right)^n,\\;occurrence\\,probability \\\\ &amp;\\left(1-\\frac{1}{MRI}\\right)^n,\\;non-occurrence\\,probability \\end{split} \\end{cases} } \\] Figure 2.7: Compound Probability If it is consider exposure time as a multiple of return period, the resulting figure 2.7, shows that: When exposure time is .69% of the return period, then probability (occurrence and non-occurrence) will be 50% As was stated previously, when exposure time is equal to return period, then the probability that the extreme wind speed (return level) occur is 63%, and 37% for the non occurrence probability. If exposure time is 4.5 times the return period, there is a 99% of change that the return level will occur. The example discussed here was presented as an instrument to introduce important concepts, nonetheless,there are specialized approaches to deal with extreme value analysis which will be discussed in Extreme Value Analysis Overview and more in detail in Peaks Over Threshold - Poisson Process. In summary, is necessary to fit the data over a specific threshold to an extreme value distribution, and \\(P_e\\) will be \\(1-F(y)\\), with F(y) as the cdf, and MRI as \\(\\frac{1}{1-F(y)}\\). 2.3 Extreme Value Analysis Overview Analysis of extreme values is related with statistical inference to calculate probabilities of extreme events. Main methods to analyze extreme data are epochal, Peaks Over Threshold - POT, and extreme index. The epochal method, also known as block maxima, uses the most extreme value for a specific frame of time, typically, one year. POT is based in the selection of a single threshold value to do the analysis only with values above the threshold. But there are different POT approaches, the most common one is Generalized Pareto Distribution - POT-GPD, but also it is possible to use the Poisson process approach. In both methods (Epochal and POT), the first step is to fit the data to an appropriate probability distribution model, among them the most used are, - Extreme Value Type I (Gumbel), Extreme Value Type II (Fréchet), Weibull, Generalized Pareto - GPD, and Generalized Extreme Value - GEV. Distribution models are fitted based in the estimation of its parameters, commonly called location, scale and shape, nonetheless each model has its own parameters names. There are different methods to estimate parameters, among them, - method of moments (modified moments - see (???), and L moments - see (???)), - method of maximum likelihood MLE, see (???), which is problematic for GPD and GEV, - probability plot correlation coefficient, and - elemental percentiles (for GPD and GEV) Once candidate parameters are available, it is necessary to assess the goodness of fit of the selected model, using one of the next methods, - Kolmogorov-Smirnov (KS) goodness of fit test, and - Anderson-Darling goodness of fit test. Here a visual assessment is also useful using a probability plot or a kernel density plot with the fitted pdf overlaid. The main use of the fitted model is the estimation of mean return intervals - MRI, and extreme wind speeds (return levels), \\[ MRI=\\frac{1}{1-F(y)} \\] with \\(F(y)\\) as the cdf. If \\(1-F(y)\\) is the annual exceedance probability, MRI is its inverse, see (???) for more details about MRI. If \\(y\\) is solved from previous equation using a given MRI of N-years, its value represents the \\(Y_N\\) wind speed return level, \\[ Y_N = G\\left(1-\\frac{1}{\\lambda\\,N}\\right) \\] where \\(G\\) is the ppf (quantile function) and \\(\\lambda\\) is the number of wind speeds over the threshold per year. The CRAN Task View “Extreme Value Analysis” https://cran.r-project.org/web/views/ExtremeValue.html shows available R for block maxima, POT by GPD, and external indexes estimation approaches. Most important to consider are evd, extremes, evir, POT, extremeStat, ismev, and Renext. 2.4 Peaks Over Threshold - Poisson Process According to (???) the stochastic Poisson process is mainly defined by its intensity function. As the intensity function is nos uniform over the domain, the Poisson process considered here is non-homogeneous, and due to the intensity function dependency of magnitude and time, it is also bi-dimensional. Poisson Process was described for the first time in (???), then extended in (???). \\[\\begin{equation} \\mathrm{ \\lambda\\left(y,t\\right) \\begin{cases} \\begin{split} &amp;\\lambda_t(y),\\;for\\,t\\,in\\,thunderstorm\\,period \\\\ &amp;\\lambda_nt(y),\\;for\\,t\\,in\\,non-thunderstorm\\,period \\end{split} \\end{cases} } \\tag{2.5} \\end{equation}\\] Generic equation (2.5) shows the intensity function, which is defined in the domain \\(D = D_t\\,{\\cup}\\,D_{nt}\\), and allow to fit the Poisson process at each station to the observed data \\(\\{t_i, y_i\\}_{i=1}^I\\) for all the times (\\(t_i\\)) of threshold crossing observations and its corresponding wind speeds magnitudes (\\(y_i\\)). Thus, only data above the threshold is used. Intensity function of the Poisson Process is defined in (???), \\[ \\frac{1}{\\psi_t}\\left(1+\\zeta_t\\frac{y-\\omega_t}{\\psi_t}\\right)_+^{-\\frac{1}{\\zeta_t}-1} \\] Where \\(\\zeta_t\\) controls the tail length of the intensity function at a given time \\(t\\), but to facilitate the estimation of the parameters then \\(\\zeta_t\\) is taken to be zero, then doing the limit, the resulting intensity function is the same as the the GEV type I or Gumbel distribution, \\[ \\frac{1}{\\psi_t}\\exp\\left\\{\\frac{-(y-\\omega_t)}{\\psi_t}\\right\\} \\] In this study, the used intensity functions are shown in equation (2.6). \\[\\begin{equation} \\mathrm{ \\lambda\\left(y,t\\right) \\begin{cases} \\begin{split} &amp;\\frac{1}{\\psi_s}\\exp\\left\\{\\frac{-(y-\\omega_s)}{\\psi_s}\\right\\},\\;for\\,t\\,in\\,thunderstorm\\,period \\\\ &amp;\\frac{1}{\\psi_{nt}}\\exp\\left\\{\\frac{-(y-\\omega_{nt})}{\\psi_{nt}}\\right\\},\\;for\\,t\\,in\\,non-thunderstorm\\,period \\end{split} \\end{cases} } \\tag{2.6} \\end{equation}\\] Figure 2.8: Domain off the Poisson Process Figure 2.8 represent the domain \\(D\\) of the Poisson process. In time, the domain represents the station service period from first sample \\(t_1\\) to last sample \\(t_4\\). \\(D\\) is the union of all thunderstorm periods \\(D_t\\) (from \\(t_2\\) to \\(t_3\\)), and all non-thunderstorm periods \\(D_{nt}\\) (periods \\(t_1\\) to \\(t_2\\) and \\(t_3\\) to \\(t_4\\)). In magnitude, only thunderstorm data above its threshold \\(b_t\\), and only non-thunderstorm data above its threshold \\(b_{nt}\\) are used. Thunderstorms and non-thunderstorms are modeled independently: Observations in domain \\(D\\) follow a Poisson distribution with mean \\(\\int_D\\lambda(t,y)\\,dt\\,dy\\) For each disjoint sub-domain \\(D_1\\) or \\(D_2\\) inside \\(D\\), the observations in \\(D_1\\) or \\(D_2\\) are independent random variables. Visual representation of the intensity function for the Poisson Process can be seen in figure 2.9. In vertical axis, two surfaces were drawn representing independent intensity functions for thunderstorm \\(\\lambda_t(y)\\) and for non-thunderstorm \\(\\lambda_{nt}(y)\\). The volume under each surface for its corresponding time periods and peak (over threshold) velocities, is the mean of the Poisson Process. Figure 2.9: Volume under surfaces represents the mean of the Poisson process The method of maximum likelihood es used to estimate the parameters of the Poisson process, the selected vector of parameters \\(\\eta\\) are the \\(\\hat\\eta\\) values that maximizes the function \\[\\begin{equation} \\mathrm{ L(\\eta)=\\left( \\prod_{i=1}^I\\lambda\\left(y_i,t_i\\right) \\right) \\exp\\left\\{ -\\int_{{D}}\\lambda\\left(y,t\\right)dy\\,dt \\right\\} } \\tag{2.7} \\end{equation}\\] \\(\\hat\\eta\\) values need to be calculated using a numerical approach because there is not analytical solution available. Once the Poisson process is fitted to the data, the model will provide extreme wind velocities (return levels), for different return periods (mean recurrence intervals). A \\(Y_N\\) extreme wind velocity, called the return level (RL) belonging to the N-years return period, has a expected frequency to occur or to be exceeded (annual exceedance probability)\\(P_e = \\frac{1}{N}\\), and also has a probability that the event does not occur (annual non-exceedance probability) \\(P_{ne}=1-\\frac{1}{N}\\). \\(Y_N\\) will be the resulting value of the \\(G\\) (ppf or quantile) function using a probability equal to \\(P_{ne}\\). \\(Y_N=quantile(y, p=P_{ne})=G(x,p=P_{ne})=ppf(x,p=P_{ne})\\). As for this study \\(\\zeta = 0\\), the \\(G\\) function to use is the Gumbel quantile function. \\(Y_N\\) can be understood as the wind extreme value expected to be exceeded on average once every N years. For different POT approaches, as POT-GPD described –, the value of the probability passed to the \\(G\\) function, has to be modified with the \\(\\lambda\\) parameter, as is described in next equation. \\(\\lambda\\) is the number of wind speed over the threshold per year. \\[ Y_N =G\\left(y, 1-\\frac{1}{\\lambda\\,N}\\right) \\] For the Poisson process \\(Y_N\\) is also the solution to the next equation, which is defined in terms of the intensity function, \\[\\begin{equation} \\mathrm{ \\int_{Y_N}^{\\infty}\\int_0^1\\lambda\\left( y,t\\right)dydt = A_t\\int_{Y_N}^{\\infty}\\lambda_t\\left( y\\right)\\,dy + A_{nt}\\int_{Y_N}^{\\infty}\\lambda_{nt}\\left( y\\right)\\,dy = \\frac{1}{N} } \\tag{2.8} \\end{equation}\\] where \\(A_t\\), is the multiplication of the average number of thunderstorm per year and the average length of a thunderstorm (taken to be 1 hour as defined in (???)), and \\(A_{nt} = 1 - A_t\\). The average length of a non-thunderstorm event is variable, and it is adjusted in each station to guarantee that \\(A_{nt} + A_t = 1\\) The same thunderstorm event in considered to occur if the time lag distance between successive thunderstorm samples is small than six hours, and for non-thunderstorm this time is 4 days. For the Poisson process, all the measurements belonging to the same event (thunderstorm or non-thunderstorm), need to be de-clustered to leave only one maximum value. In other words, the number of thunderstorm in the time series is the number of time lag distances grater than 6 hours, and for non-thunderstorm grater than 4 days. 2.4.1 Threshold Selection \\[U=F(Y)\\] \\[W = -log(1-U)\\] 2.5 Wind Loads Requirements As the output maps of this research will be used as input loads for infrastructure design, the methodology used for its creation, need to be consistent with Colombian official wind loads requirements. Today (2020), the Colombian norm that defines wind loads is the Seismic Resistant Standard 2010 - NSR-10 by its acronym of Spanish, see XXX. Chapter related to wind loads is B.6. NSR-10 was created and is maintained by the Colombian Association of Seismic Engineering - AIS. NSR-10 is mainly based in the USA norm American Society of Civil Engineers 7-16, minimum design loads and associated criteria for buildings and other structures - ASCE7-16, see (???). Under these circumstances, ASCE7-16 defines the minimum requirements of the research products. Especially the chapter C26 - “wind loads - general requirements”, C26.5 “wind hazard map”, and C26.7 “Exposure” - pages 733 to 747. Wind speeds requirements of ASCE7-16 are based in the combination of independent non-hurricane analysis, and hurricane wind speeds simulations models. The focus of this research will be the analysis of non-hurricane wind data, however, existing results of hurricane studies will be used to present final maps with both components. In ASCE7-16, for non-hurricane wind speed, the procedure is mainly based on (???). ASCE7-16 (page 734), requires the calculation of wind extreme return levels for specific return periods according to the risk category of the structure to be designed: risk category I - 300 years, risk category II - 700 years, risk category III - 1700 years, risk category IV - 3000 years. NSR-10 only requires 700, 1700 and 3000 years. In addition, extreme wind speeds for those MRI need to correspond to: - 3 second gust speeds, - at 33 ft (10 meters) above the ground, and - exposure category C (open space). Risk IV - This are ‘indispensable buildings’ that involve substantial risk. This structures that can handle toxic or explosive substances. Risk III - There is substantial risk because this structures that can handle toxic or explosive substances, can cause a serious economical impact, or massive interruption of activities if they fail Risk II - Category ‘by default’, and correspond to structures not classified in others categories. Risk I - This structures represent low risk for people lives To standardize wind speeds to gust speeds ASCE7-16 proposes the curve Durst (see (???), and figure 2.10). It is valid only for open terrain conditions. Durst curve shows in axis \\(y\\) the gust factor \\(\\frac{V_t}{V_{3600}}\\), a ration between any wind gust averaged at \\(t\\) seconds, \\(V_t\\), and the hourly averaged wind speed \\(V_{3600}\\), and in the axis \\(x\\) the duration \\(t\\) of the gust. Figure 2.10: Maximum speeds averaged over t (sec), to hourly mean speed "],
["3-rmd-method.html", "Chapter 3 Methodology 3.1 Data 3.2 POT - Poisson Process 3.3 Spatial Interpolation 3.4 Integration with Non-Hurricane data", " Chapter 3 Methodology Figure 3.1 shows a graphic representation of the methodology. This thesis research is focus in non-hurricane data, with three main elements: - data, - temporal analysis with a POT- Poisson process, and - spatial analysis with probabilistic and deterministic methods to do spatial interpolation and create return levels (wind velocities) maps, for MRI of 700, 1700, and 3000 years. An additional element, is the integration with existing hurricane maps to produce final maps, that will be used as input loads for infrastructure design, and will be part of the NSR-10. More representative and important steps of the methodology are identified by numbers, 1) standardization, 2) de-clustering, 3) thresholding, 4) fit intensity function, 5) hazard curve, 6) return levels, and 7) spatial interpolation. Steps 1 to 6, need to be done for each available station to get MRI wind velocities. With MRI wind velocities in each station, a continuous surface will be created, one for 700 years, next for 1700 years and finally 3000 years. Figure 3.2 schematize that iterative process. Figure 3.1: Methodology Figure 3.2: Iterative process in methodology 3.1 Data 3.1.1 Getting the data 3.1.2 Exploratory Data Analysis - EDA 3.1.3 Standardization 3.1.3.1 Anemometer height - 10 m According to the protocol for field data collection and location of methodological stations - IDEAM, 2005 (XXX), the anemometer (wind sensor) in installed always to a fixed high of 10 meters from the surface, as is shown in figure XXX 3.1.3.2 Surface Roughness - 0.03 m 3.1.3.3 Averaging Time - 3-s gust To transform hourly mean wind velocity (\\(V_{3600}\\)), ASCE7-16 (XXX) recommended to use Durst 1960 (XXX). 3.1.4 Filtering 3.1.5 Separating 3.1.6 Comparison 3.1.7 Selection of input data 3.2 POT - Poisson Process 3.2.1 De-clustering 3.2.2 Exclude no-data periods 3.2.3 Thresholding As the POT model requires to work only with the most extreme values in the time series, it is necessary to select a threshold to filter out small values. Selection of threshold value imply two effects in the model. Bias is high when a low threshold is selected (many exceedances) because the asymptotic support is weak. Opposite situation happens for high thresholds where variance is potentially high, so according to Davidson and Smith 1990 (XXX), it is needed to select a threshold value, consistent with model structure. 3.2.4 Fit Intensity Function 3.2.4.1 Intensity function 3.2.4.2 Density function 3.2.4.3 Distribution function 3.2.4.4 Maximum likelihood estimation 3.2.5 Hazard Curve 3.2.6 Return Levels 3.3 Spatial Interpolation 3.3.1 Kriging 3.3.2 IDW 3.3.3 Local Polynomials 3.4 Integration with Non-Hurricane data "],
["conclusion.html", "Conclusion", " Conclusion If we don’t want Conclusion to have a chapter number next to it, we can add the {-} attribute. More info And here’s some other random info: the first paragraph after a chapter title or section head shouldn’t be indented, because indents are to tell the reader that you’re starting a new paragraph. Since that’s obvious after a chapter or section title, proper typesetting doesn’t add an indent there. If you feel it necessary to include an appendix, it goes here. --> "],
["A-r-code.html", "A R Code", " A R Code This first appendix includes all of the R chunks of code that were hidden throughout the document (using the include = FALSE chunk tag) to help with readability and/or setup. In the main Rmd file # This chunk ensures that the thesisdown package is # installed and loaded. This thesisdown package includes # the template files for the thesis. if(!require(devtools)) install.packages(&quot;devtools&quot;, repos = &quot;http://cran.rstudio.com&quot;) if(!require(thesisdown)) devtools::install_github(&quot;ismayc/thesisdown&quot;) library(thesisdown) In Chapter 3: # This chunk ensures that the thesisdown package is # installed and loaded. This thesisdown package includes # the template files for the thesis and also two functions # used for labeling and referencing if(!require(devtools)) install.packages(&quot;devtools&quot;, repos = &quot;http://cran.rstudio.com&quot;) if(!require(dplyr)) install.packages(&quot;dplyr&quot;, repos = &quot;http://cran.rstudio.com&quot;) if(!require(ggplot2)) install.packages(&quot;ggplot2&quot;, repos = &quot;http://cran.rstudio.com&quot;) if(!require(ggplot2)) install.packages(&quot;bookdown&quot;, repos = &quot;http://cran.rstudio.com&quot;) if(!require(thesisdown)){ library(devtools) devtools::install_github(&quot;ismayc/thesisdown&quot;) } library(thesisdown) flights &lt;- read.csv(&quot;data/flights.csv&quot;) "],
["B-the-second-appendix.html", "B The Second Appendix", " B The Second Appendix "],
["references.html", "References", " References "]
]

[
["index.html", "Introduction 0.1 Background 0.2 Research Aim and Objectives 0.3 Reseach Question 0.4 Thesis Document Structure", " Introduction This research aims to create non-hurricane non-tornadic maps of extreme wind speeds for three specific recurrence intervals (700, 1700, and 3000 years) covering the Colombian territory. These maps will be combined with existing hurricane wind speed studies, to be used as input loads due to wind for infrastructure desing. For each station with wind speeds time histories in the input data, extreme wind speed corresponding to each recurrence interval are calculated using a Peaks Over Threshold onwards POT extreme value model, then wind velocities with the same recurrence interval are spatially interpolated to generate continuous maps for the whole study area. A wind speed linked to a mean recurrence interval - MRI of N-years (N-years return value or return period) is interpreted as the highest probable wind speed along the period of N-years. The annual probability of equal or exceed that wind speed is 1/N. The annual excedance probability for all velocity values in 700-years output map will be 1/700, for the 1700-years map will be 1/1700, and 1/3000 for the 3000-years final map. There are different methods to model extreme value data, among them are a) sample maxima using a Generalized Extreme Value Distribution onwards GEVD (traditional method), b) POT using a Generalized Pareto Distribution onwards GPD, c) POT using a two-dimensional Poisson Process, that can be homomegenos, non-homogeneous, stationary, and non-stationary (originally know as Point Process approach), and d) POT Poisson-GPD. Following (???) in this research a POT using a non-homogeneous non-stationary two-dimensional poisson proces was selected, despide there is no R package available to apply this approach. 0.1 Background 0.2 Research Aim and Objectives 0.3 Reseach Question 0.4 Thesis Document Structure "],
["1-rmd-data.html", "Chapter 1 Data 1.1 IDEAM 1.2 ISD 1.3 ERA5 1.4 Data Download and Organization 1.5 Data Standarzation", " Chapter 1 Data Input data is made up of three different sources a) IDEAM - Institute of Hydrology, Meteorology and Environmental Studies of Colombia http://www.ideam.gov.co, b) ISD - Integrated Surface Database https://www.ncdc.noaa.gov/isd, and c) ERA5 climate reanalysis https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5. Table 1.1: Datasets description Institution Dataset Details IDEAM Historical records at weather stations IDEAM is responsible for the instalation, maintenance and management of all kind of weather stations located everywhere along the country NOAA ISD ISD (Integrated Surface Database. NOAA’s National Centers for Environmental Information - NCEI) Lite: A subset from the full ISD dataset containing eight common surface parameters in a fixed-width format free of duplicate values, sub-hourly data, and complicated flags. ECMWF ERA5 ERA5 is a reanalysis dataset with hourly estimates of atmospheric variables with horizontal resolution of 0.25º (33 kilómeters), this is equally spaced cells every 0.25 degrees Table 1.2: Datasets variables Dataset Variables Description IDEAM vvmx_aut_60 Hourly wind maximun velocity ISD wind speed rate Maximun hourly wind velocity. The rate of horizontal travel of air past a fixed point. ERA5 fg10 10 metre wind gust since previous post-processing fsr Forecast Surface Roughness Table 1.3: Variables units and time Variable Units Time Stations vvmx_aut_60 meters per second Variable from 2001 until today. Irregular time series. 203 Wind speed meters per second Variable from 1941 until today. Note: There is too much variability in time (start, end, and time range) for each station. Irregutal time series. 101 fg10 meters per second 1979-Today 3381 fsr meters per second 1979-Today 3381 Ideal data source to create extreme wind speeds maps should be field observed data from IDEAM, but there are not enough number of stations around the study area to represent all the local wind variability in a huge country with multiple variety of climates and and changing thermal floors, but there are other important motivatios to include different sources trying to improve output results: As just mentioned, low quantity of IDEAM stations There are uncertanties related to the way IDEAM anemometers are registering data, then comparison with other datasources are needed to be able to do appropriate data standardization, needed as a prerequisite to the analysis. There is no time continuity in the registration of IDEAM data. Historical time series are different and variable in each station. Importance of ISD database for this study is based on the fact that post-procesed ISD database has wind extreme values, and it was used to create extreme wind maps for United States. ISD allows comparison with IDEAM records to take better decitions in order to do needed data standarization. Despite that ERA5 data are not observed data, but forecast, its main advantage is data availability to assess the local climatic variance every 33 square kilometers. 1.1 IDEAM Historical observed wind speeds from 203 in Colombia are managed by the official environmental authority IDEAM. Table 1.4 shows a sample of five IDEAM stations. Figure 1.1 shows a map of IDEAM stations. Table 1.4: IDEAM Stations sample Name[Code] Latitud Longitud EMAS - AUT [26155230] 5.09 -75.51 SAN BENITO - AUT [25025380] 9.16 -75.04 AEROPUERTO ALFONSO LOPEZ - [28025502] 10.44 -73.25 TIBAITATA - AUT [21206990] 4.69 -74.21 ELDORADO CATAM - AUT [21205791] 4.71 -74.15 Figure 1.1: IDEAM Stations Following, the time serie, autocorrelation function, and partial autocorrelation function, for IDEAM station “21205791” will be displayed. Figure 1.2: IDEAM Station - Time Serie Figure 1.3: IDEAM Station ACF Figure 1.4: IDEAM Station PACF 1.2 ISD ISD is a database with environmental variables among then extreme wind speeds. ISD has data for the whole planet, and is based on observed data at metereological stations in each country, which means that for Colombia is based on IDEAM data. Main advantage is data availability at neighbor countries and specialized postprocesing made by NOAA’s National Centers for Environmental Information - NCEI in United States, which facilitates its use.Table 1.5 shows a sample of five ISD stations. Figure 1.5 shows a map of ISD stations. Table 1.5: ISD Stations sample Code Name Latitud Longitud 804400 BARINAS 8.62 -70.22 800810 ALTO CURICHE 7.05 -76.35 801000 BAHIA SOLANO / JOSE MUTIS 6.18 -77.40 802590 ALFONSO BONILLA ARAGON INTL 3.54 -76.38 803150 BENITO SALAS 2.95 -75.29 Figure 1.5: ISD Stations Following, the time serie, autocorrelation function, and partial autocorrelation function, for ISD station “802590” will be displayed. select &quot;mydatetime&quot;, &quot;802590&quot; as &quot;X802590&quot; from isd_lite_unstack where &quot;802590&quot; IS NOT NULL Figure 1.6: ISD Station - Time Serie Figure 1.7: ISD Station ACF Figure 1.8: IDEAM Station PACF 1.3 ERA5 ERA5 is forecast reanalysis data procesed by the European Centre for Medium-Range Weather Forecasts - ECMWF with wind speeds time series in square cells matrix of pixels of 0.25 degrees (33 km) covering the whole plannet. For the study area was extracted a raster of 69 rows by 49 XXX columns in format NetCDF. Figure 1.9 shows a map of ERA5 stations (cells centers). Figure 1.9: ERA5 Stations (cells centers) 1.4 Data Download and Organization 1.5 Data Standarzation Analysis of extreme wind speeds requieres data standarizaton as initial step. All input data must be standarized to represent three important conditions: a) anemometer height of 10 meters, b) open space roughness, and c) averaging time of 3-seconds wind gust. Data for analysis must represent 3-s peak wind speeds 10 meters heigh above the surface, in open terrain. * 10 mts anemometer height * Open space terrain roughness * 3-s gust averaging time "],
["2-rmd-thefra.html", "Chapter 2 Theoretical Framework 2.1 Probability Concepts 2.2 Statistical Concepts For Extreme Analysis 2.3 Extreme Value Analysis Overview 2.4 Peaks Over Threshold - Poisson Process 2.5 Wind Loads Requirements", " Chapter 2 Theoretical Framework 2.1 Probability Concepts Poisson process is an stochastic method that relies in the concepts of probability distributions. The main functions related to probability for extreme value analysis will be described below. 2.1.1 Probability Density Function - pdf Pdf defines the probability that a continuos variable falls between two points, this is, in pdf the proability is related to the area below the curve (integral) between two points, as for continuos probability distributions the probability at a single point is zero. The term density is directly related to the probability of a portion of the curve, if the density function has high values the probability will be greater in comparison with the same portion of curve for low values. \\[ \\int_a^b f(x)dx = Pr[a \\leq X \\leq b] \\] Equation (2.1) is the Gumbel pdf. \\[\\begin{equation} \\mathrm{ f(x)=\\frac{1}{\\beta} \\exp\\left\\{ -\\frac{x-\\mu}{\\beta} \\right\\} \\exp\\left\\{ -\\exp\\left\\{ -\\left( \\frac{x-\\mu}{\\beta} \\right) \\right\\} \\right\\}, \\quad -\\infty &lt; x &lt; \\infty } \\tag{2.1} \\end{equation}\\] where \\(\\exp\\left\\{.\\right\\} \\mapsto \\mathrm{e}^{\\left\\{.\\right\\}}\\), \\(\\beta\\) is the scale parameter, and \\(\\mu\\) is the location parameter. Location (\\(\\mu\\)) has the effect to shift the pdf to left or right along ‘x’ axis, thus, if location value is changed the effect is a movement of pdf to the left (small value for location), or to the right (big value for location). Scale has the effect to stretch (\\(\\beta &gt; 1\\)) of compress (\\(0 &lt; \\beta&lt; 1\\)) the pdf, if scale parameter is close to zero the pdf approaches a spike. Figure 2.1 shows pdf with location (\\(\\mu\\)) = 100 and scale (\\(\\beta\\)) = 40, using equation (2.1). location = 100 scale = 40 .x &lt;- seq(0, 300, length.out=1000) pdfG &lt;- function(x) { 1/location *exp(-(x-location)/scale)*exp(-exp(-(x-location)/scale)) } .y = pdfG(.x) plot(.x, .y, col=&quot;green&quot;, lty=4, xlab=&quot;Velocities Km/h&quot;, ylab=&quot;Density Function - Gumbel Distribution&quot;, main=paste(&quot;Gumbel - Density Function Gumbel Distribution\\n&quot;, &quot;Location=&quot;, round(location,2), &quot; Scale=&quot;, round(scale,2)), type=&quot;l&quot;, cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6) Figure 2.1: Gumbel pdf Figure 2.2 shows pdf with location (\\(\\mu\\)) = 100 and scale (\\(\\beta\\)) = 40, using function dgumbel of the package RcmdrMisc location = 100 scale = 40 .x &lt;- seq(0, 300, length.out=1000) dfG = dgumbel(.x, location=location, scale=scale) plot(.x, dfG, col=&quot;red&quot;, lty=4, xlab=&quot;Velocities Km/h&quot;, ylab=&quot;Density Function - Gumbel Distribution&quot;, main=paste(&quot;Gumbel - Density Function Gumbel Distribution\\n&quot;, &quot;Location=&quot;, round(location,2), &quot; Scale=&quot;, round(scale,2)), type=&quot;l&quot;, cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6) Figure 2.2: Gumbel pdf - dgumbel function 2.1.2 Cumulative Distribution Funtcion - cdf Cdf is the probability of taking a value less than or equal to x. That is \\[ F(x) = Pr[X &lt; x] = \\alpha \\] For a continuous variable, cdf can be expressed as the integral of its pdf. \\[ F(x) = \\int_{-\\infty}^x f(x)dx \\] Equation (2.2) is the Gumbel cdf. \\[\\begin{equation} \\mathrm{ F(x) = \\exp\\left\\{-\\exp\\left[-\\left(\\frac{x-\\mu}{\\beta}\\right)\\right]\\right\\}, \\quad -\\infty &lt; x &lt; \\infty } \\tag{2.2} \\end{equation}\\] Figure 2.3 shows Gumbel cdf with location (\\(\\mu\\)) = 100 and scale (\\(\\beta\\)) = 40, using equation (2.2). As previously done with pdf, similar result can be achieved using function pgumbel of package RcmdrMisc. location = 100 scale = 40 .x &lt;- seq(0, 300, length.out=1000) cdfG &lt;- function(x) { exp(-exp(-(x-location)/scale)) } .y = cdfG(.x) plot(.x, .y, col=&quot;green&quot;, lty=4, xlab=&quot;Velocities Km/h&quot;, ylab=&quot;Probability&quot;, main=paste(&quot;Gumbel - Cumulative Distribution Function\\n&quot;, &quot;Location=&quot;, round(location,2), &quot; Scale=&quot;, round(scale,2)), type=&quot;l&quot;, cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6) Figure 2.3: Gumbel cdf 2.1.3 Percent Point Function - ppf Ppf is the inverse of cdf, also called the quantile function. This is, from a specific probability get the corresponding value x of the variable. \\[ x = G(\\alpha) = G(F(x)) \\] Equation (2.3) is the Gumbel ppf. \\[\\begin{equation} \\mathrm{ G(\\alpha) = \\mu-\\beta ln(-ln(\\alpha)) \\quad 0 &lt; \\alpha &lt; 1 } \\tag{2.3} \\end{equation}\\] Figure 2.4 shows Gumbel ppf, using equation (2.3). Similar result can be achieved using function qgumbel of package RcmdrMisc. location = 100 scale = 40 .x &lt;- seq(0, 1, length.out=1000) ppfG &lt;- function(x) { location - (scale*log(-log(x))) } .y = ppfG(.x) plot(.x, .y, col=&quot;green&quot;, lty=4, ylab=&quot;Velocities Km/h&quot;, xlab=&quot;Probability&quot;, main=paste(&quot;Gumbel - Percent Point Function\\n&quot;, &quot;Location=&quot;, round(location,2), &quot; Scale=&quot;, round(scale,2)), type=&quot;l&quot;, cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6) Figure 2.4: Gumbel cdf 2.1.4 Hazard Function - hf Using \\(S(x) = 1 - F(x)\\) as survival function -sf, the probability that a variable takes a value greather than x \\(S(x) = Pr[X &gt; x] = 1 - F(x)\\), the hf is the ratio between pdf and sf. \\[ h(x) = \\frac{f(x)}{S(x)} = \\frac{f(x)}{1-F(x)} \\] Equation (2.4) is the Gumbel ppf. \\[\\begin{equation} \\mathrm{ h(x)= \\frac{1}{\\beta}\\frac{\\exp(-(x-\\mu)/\\beta)}{\\exp(\\exp(-(x-\\mu)/\\beta))-1} } \\tag{2.4} \\end{equation}\\] Figure 2.5 shows Gumbel hf, using equation (2.4). location = 100 scale = 40 .x &lt;- seq(0, 3000, length.out=1000) hfG &lt;- function(x) { (1/scale)*(exp(-(x-location)/scale))/(exp(exp(-(x-location)/scale))-1) } .y = hfG(.x) plot(.x, .y, col=&quot;green&quot;, lty=4, xlab=&quot;Velocities Km/h&quot;, ylab=&quot;Hazard&quot;, main=paste(&quot;Gumbel - Hazard Function\\n&quot;, &quot;Location=&quot;, round(location,2), &quot; Scale=&quot;, round(scale,2)), type=&quot;l&quot;, cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6) Figure 2.5: Gumbel cdf #library(reliaR) #plot(.x, hgumbel(.x, mu=location, sigma=scale)) #plot(.x, hra.gumbel(.x, mu=location, sigma=scale)) 2.2 Statistical Concepts For Extreme Analysis In order to approach the extreme value analysis, some statistical concepts are needed to understand the theoretical framework behind this knowledge area. In this section will be introduced the concepts annual excedance probability, mean recurrence interval - MRI, exposure time, and compound probability for any given exposure time and MRI. As an hypotetical example, a simulated database of extreme wind speed will be used. This database is supposed to have 10.000 years of simulated wind speeds. 2.2.1 Annual Excedance Probability - Pe Using the previously described database, a question arises to calculate the probability to exceed the highest probable loss due to the simulated winds. It is possible to conclude that there is only one event grather or equal (in this case equal) to the higest probable causing loss in 10.000 years, and it is the highest wind. If we sort the database by wind magnitude in descending order (small winds last), the question is solved calculating the annual excedance probability Pe with next formula Figure 2.6: Sorted Winds by Magnitud - wind simulation database \\[ P_e = \\frac{Event\\,index\\,after\\,descending\\,sorting}{Years\\,of\\,simulations } = \\frac{1}{10.000}=0.001=0.01\\% \\] because the highest wind will be the first in the sorted list. Same exercise can be done with all winds to construct the annual exedance probability curve, that in this case will represent the probability to equal or exceed different probable losses due to wind. 2.2.2 Return Period - Mean Recurrence Interval Continuing with the previous section, if the inverse of the excedance probability is taken, the return period (in years) is obtained. The return period or Mean Recurrence Interval - MRI is associated with an specific return level (wind extreme velocity). MRI is the numbers of years (N) needed to obtain 63% of change that the corresponding return level will occur at least one time in that period. The return level is expected to be exceeded on average once every N-years. The annual excedance probability of the return level corresponding to N-years of MRI, is \\(P_e=\\frac{1}{MRI}=\\frac{1}{N}\\). For an specific wind extreme event A, the probability that the event will occur in a period equal to MRI years is 63%. If we analyse for the same period a strongest wind extreme event B, its orrurence probability will be lest than 67%. If the purpose of this researh is to design infrastructure considering wind loads, the structure will be more resistant to wind if we desing with stronger winds, this is high MRIs, and low annual excedance probability. Common approach for infrastructure design, considering any type of load (earthquake, wind, etc) is to choose high MRI according to the importance/use/risk/type of the structure. For highly important structures lik hospitals or coliseums, where the risk of collapse must be diminished, the MRI used to design is higher in comparison to common structures (for instance a normal house), which implies less risks for its use and importance. \\[ \\mathrm{ P_e = \\begin{cases} \\begin{split} &amp;1-\\exp\\left(-\\frac{1}{MRI}\\right),\\;for\\,MRI\\,&lt;\\,10\\,years \\\\ &amp;\\frac{1}{MRI},\\;for\\,MRI\\,\\geq\\,10\\,years \\end{split} \\end{cases} } \\] 2.2.3 Compound Excedance Probability - Pn If time of exposure is consider, understood as time the structure will be in use, it is possible to have a compound probability \\(P_n\\), where \\(n\\) is the exposure period. \\(P_n\\) is the probability that the extreme wind speed will be equaled or exceeded at least one time in \\(n\\) years, and is related with the occurrence probability, but also is posible to calculate the non-ocurrence compound probability (probability that the event will not occur). \\[ \\mathrm{ P_n = \\begin{cases} \\begin{split} &amp;1-\\left(1-\\frac{1}{MRI}\\right)^n,\\;occurrence\\,probability \\\\ &amp;\\left(1-\\frac{1}{MRI}\\right)^n,\\;non-occurrence\\,probability \\end{split} \\end{cases} } \\] Figure 2.7: Compound Probability If it is consider exposure time as a multiple of return period, the resulting figure 2.7, shows that: When exposure time is .69% of the return period, then probability (ocurrence and non-occurrence) will be 50% As was stated previously, whe exposure time is equal to return period, then the probability that the extreme wind speed (return level) occur is 63%, and 37% for the non occurrence probability. If exposure time is 4.5 times the return period, there is a 99% of change that the return level will occur. The example discused here was presented as an instrument to introduce important concepts, nonetheless,there are specialized approaches to deal with extreme value analysis which will be discussed in Extreme Valua Analysis Overview and more in detail in Peaks Over Threshold - Poisson Process. In summary, is neccesary to fit the data over a specific threshold to an extreme value distribution, and \\(P_e\\) wil be \\(1-F(y)\\), with F(y) as the cdf, and MRI as \\(\\frac{1}{1-F(y)}\\). 2.3 Extreme Value Analysis Overview Analysis of extreme values is related with statistical inference to calculate probabilities of extreme events. Main methods to analize extreme data are ephochal, Peaks Over Threshold - POT, and extreme index. The epochal method, also known as block maxima, uses the most extreme value for a specific frame of time, tipically, one year. POT is based in the selection of a single threshold value to do the analysis only with values above the threshold. But there are different POT aproaches, the most commond one is Generalized Paretto Distribution - POT-GPD, but also it is possible to use the Poisson process approach. In both methods (Epochal and POT), the first step is to fit the data to an appropiate probability distribution model, among them the most used are, - Extreme Value Type I (Gumbel), Extreme Value Type II (Frechet), Weibull, Generalized Pareto - GPD, and Generalized Extreme Value - GEV. Distribution models are fitted based in the estimation of its parameters, mommonly called location, scale and shape, nonetheless each model has its own parameters names. There are different methods to estimate parameters, among them, - method of moments (modified moments - see (???), and L moments - see (???)), - method of maximum likelihood MLE, see (???), which is problematic for GPD and GEV, - probability plot correlation coeficient, and - elemental percentiles (for GPD and GEV) Once cadidate parameters are available, it is neccesary to assess the goodness of fit of the selected model, using one of the next methods, - Kolmogorov-Smirnov (KS) goodnes of fit test, and - Anderson-Darling goodness of fit test. Here a visual assesment is also useful using a probability plot or a kernel density plot with the fitted pdf overlaid. The main use of the fitted model is the estimation of mean return intervals - MRI, and extreme wind speeds (return levels), \\[ MRI=\\frac{1}{1-F(y)} \\] with \\(F(y)\\) as the cdf. If \\(1-F(y)\\) is the annual excedance probability, MRI is its inverse, see (???) for more details about MRI. If \\(y\\) is solved from previos equation using a given MRI of N-years, its value represents the \\(Y_N\\) wind speed return level, \\[ Y_N = G\\left(1-\\frac{1}{\\lambda\\,N}\\right) \\] where \\(G\\) is the ppf (quantile function) and \\(\\lambda\\) is the number of wind spees over the threshold per year. The CRAN Task View “Extreme Value Analysis” https://cran.r-project.org/web/views/ExtremeValue.html shows available R for block maxima, POT by GPD, and external indexes estimation aproaches. Most important to consider are evd, extremes, evir, POT, extremeStat, ismev, and Renext. 2.4 Peaks Over Threshold - Poisson Process According to (???) the stochastic poisson process is mainly defined by its intensity function. As the intensity function is nos uniform over the domain, the poisson process considered here is non-homogeneous, and due to the intensity function dependance of magnitud and time, it is also bi-dimmensional. Poisson Process was described for the first time in (???), then extended in (???). \\[\\begin{equation} \\mathrm{ \\lambda\\left(y,t\\right) \\begin{cases} \\begin{split} &amp;\\lambda_t(y),\\;for\\,t\\,in\\,thunderstorm\\,period \\\\ &amp;\\lambda_nt(y),\\;for\\,t\\,in\\,non-thunderstorm\\,period \\end{split} \\end{cases} } \\tag{2.5} \\end{equation}\\] Generic equation (2.5) shows the intensity function, which is defined in the domain \\(D = D_t\\,{\\cup}\\,D_{nt}\\), and allow to fit the poisson process at each station to the observed data \\(\\{t_i, y_i\\}_{i=1}^I\\) for al the times (\\(t_i\\)) of threshold crossing observations and its corresponding wind speeds magnitudes (\\(y_i\\)). Thus, only data above the threshold is used. Intensity function of the Poisson Process is defined in (???), \\[ \\frac{1}{\\psi_t}\\left(1+\\zeta_t\\frac{y-\\omega_t}{\\psi_t}\\right)_+^{-\\frac{1}{\\zeta_t}-1} \\] Where \\(\\zeta_t\\) controls the tail lengh of the intensity function at a given time \\(t\\), but to facilitate the estimation of the parameters then \\(\\zeta_t\\) is taken to be zero, then doing the limit, the resulting intensity function is the same as the the GEV type I or Gumbel distribution, \\[ \\frac{1}{\\psi_t}\\exp\\left\\{\\frac{-(y-\\omega_t)}{\\psi_t}\\right\\} \\] In this study, the used intensity functions are shown in ecuation (2.6). \\[\\begin{equation} \\mathrm{ \\lambda\\left(y,t\\right) \\begin{cases} \\begin{split} &amp;\\frac{1}{\\psi_s}\\exp\\left\\{\\frac{-(y-\\omega_s)}{\\psi_s}\\right\\},\\;for\\,t\\,in\\,thunderstorm\\,period \\\\ &amp;\\frac{1}{\\psi_{nt}}\\exp\\left\\{\\frac{-(y-\\omega_{nt})}{\\psi_{nt}}\\right\\},\\;for\\,t\\,in\\,non-thunderstorm\\,period \\end{split} \\end{cases} } \\tag{2.6} \\end{equation}\\] Figure 2.8: Domanin off the Poisson Process Figure 2.8 represent the domain \\(D\\) of the Poisson process. In time, the domain represents the station service period from first sample \\(t_1\\) to last sample \\(t_4\\). \\(D\\) is the union of all thunderstorm periods \\(D_t\\) (from \\(t_2\\) to \\(t_3\\)), and all non-thunderstorm periods \\(D_{nt}\\) (periods \\(t_1\\) to \\(t_2\\) and \\(t_3\\) to \\(t_4\\)). In magnitud, only thunderstorm data above its threshold \\(b_t\\), and only non-tunderstorm data above its threshold \\(b_{nt}\\) are used. Thunderstoms and non-thunderstorms are modeled independently: Observations in domain \\(D\\) follow a Poisson distribution with mean \\(\\int_D\\lambda(t,y)\\,dt\\,dy\\) For each disjoint subdomain \\(D_1\\) or \\(D_2\\) inside \\(D\\), the observations in \\(D_1\\) or \\(D_2\\) are independent random variables. Visual representation of the intensity function for the Poisson Process can be seen in figure 2.9. In vertical axis, two surfaces were drawn representing independent intensity functions for thunderstorm \\(\\lambda_t(y)\\) and for non-thunderstorm \\(\\lambda_{nt}(y)\\). The volume under each surface for its corresponding time periods and peak (over threshold) velocities, is the mean of the Poisson Process. Figure 2.9: Volume under surfaces represents the mean of the Poisson process The method of maximun likelihood es used to estimate the parameters of the Poisson process, the selected vector of parameters \\(\\eta\\) are the \\(\\hat\\eta\\) values that maximizes the function \\[\\begin{equation} \\mathrm{ L(\\eta)=\\left( \\prod_{i=1}^I\\lambda\\left(y_i,t_i\\right) \\right) \\exp\\left\\{ -\\int_{{D}}\\lambda\\left(y,t\\right)dy\\,dt \\right\\} } \\tag{2.7} \\end{equation}\\] \\(\\hat\\eta\\) values need to be calculated using a numericall approach because there is not analytical solution available. Once the Poisson process is fittet to the data, the model will provide extreme wind velocities (return levels), for different return periods (mean recurrence intervals). A \\(Y_N\\) extreme wind velocity, called the return level (RL) belonging to the N-years return period, has a expected frequency to occur or to be exceeded (annual excedance probability)\\(P_e = \\frac{1}{N}\\), and also has a probability that the event does not occur (annual non-excedance probability) \\(P_{ne}=1-\\frac{1}{N}\\). \\(Y_N\\) will be the resulting value of the \\(G\\) (ppf or quantile) function using a probability equal to \\(P_{ne}\\). \\(Y_N=quantile(y, p=P_{ne})=G(x,p=P_{ne})=ppf(x,p=P_{ne})\\). As for this study \\(\\zeta = 0\\), the \\(G\\) function to use is the Gumbel quantile function. \\(Y_N\\) can be undestood as the wind extreme value expected to be exceeded on average once every N years. For different POT approaches, as POT-GPD described –, the value of the probability passed to the \\(G\\) function, has to be modified with the \\(\\lambda\\) parameter, as is described in next equation. \\(\\lambda\\) is the number of wind speed over the threshold per year. \\[ Y_N =G\\left(y, 1-\\frac{1}{\\lambda\\,N}\\right) \\] For the Poisson process \\(Y_N\\) is also the solution to the next equation, which is defined in terms of the intensity function, \\[\\begin{equation} \\mathrm{ \\int_{Y_N}^{\\infty}\\int_0^1\\lambda\\left( y,t\\right)dydt = A_t\\int_{Y_N}^{\\infty}\\lambda_t\\left( y\\right)\\,dy + A_{nt}\\int_{Y_N}^{\\infty}\\lambda_{nt}\\left( y\\right)\\,dy = \\frac{1}{N} } \\tag{2.8} \\end{equation}\\] where \\(A_t\\), is the multiplication of the average number of thunderstorm per year and the average lengh of a thunderstorm (taken to be 1 hour as defined in (???)), and \\(A_{nt} = 1 - A_t\\). The average length of a non-thunderstorm event is variable, and it is adjusted in each station to guarantee that \\(A_{nt} + A_t = 1\\) The same thunderstorm event in considered to occur if the time lag distance between sucesive thunderstorm samples is small than six hours, and for non-thunderstorm this time is 4 days. For the Poisson process, all the measurements belonging to the same event (thunderstorm or non tunderstorm), need to be declustered to leave only one maximun value. In other words, the number of thunderstorm in the time serie is the number of time lag distances grather than 6 hours, and for non-thunderstorm grather than 4 days. ###Threshold Selection \\[U=F(Y)\\] \\[W = -log(1-U)\\] 2.5 Wind Loads Requirements As the output maps of this research will be used as input loads for infrastructure design, the methodology used for its creation, need to be consistent with Colombian official wind loads requirements. Today (2020), the Colombian norm that defines wind loads is the Seismic Resistant Standard 2010 - NSR-10 by its acronym of Spanish, see XXX. Chapter related to wind loads is B.6. NSR-10 was created and is maintained by the Colombian Association of Seismic Engineering - AIS. NSR-10 is mainly based in the USA norm American Society of Civil Engineers 7-16, minimum design loads and associated criteria for buildings and other structures - ASCE7-16, see (???). Under these circumstances, ASCE7-16 defines the minimum requirements of the research products. Especially the chapter C26 - “wind loads - general requirements”, C26.5 “wind hazard map”, and C26.7 “Exposure” - pages 733 to 747. Wind speeds requirements of ASCE7-16 are based in the combination of independent non-hurricane analysis, and hurricane wind speeds simulations models. The focus of this research will be the analysis of non-hurricane wind data, however, existing results of hurricane studies will be used to present final maps with both components. In ASCE7-16, for non-hurricane wind speed, the procedure is mainly based on (???). ASCE7-16 (page 734), requires the calculation of wind extreme return levels for specific return periods according to the risk category of the structure to be designed: risk category I - 300 years, risk category II - 700 years, risk category III - 1700 years, risk category IV - 3000 years. NSR-10 only requires 700, 1700 and 3000 years. In addition, extreme wind speeds for those MRI need to correspond to: - 3 second gust speeds, - at 33 ft (10 meters) above the ground, and - exposure category C (open space). Risk IV - This are ‘indispensable buildings’ that involve sustancial risk. This structures that can handle toxic or explosive substances. Risk III - There is sustancial risk because this structures that can handle toxic or explosive substances, can cause a serious economical impact, or masive interruption of activities if they fail Risk II - Category ‘by default’, and correspond to structures not classified in others categories. Risk I - This structures represent low risk for people lifes To standarize wind speeds to gust speeds ASCE7-16 proposes the curve Durst (see (???), and figure 2.10). It is valid only for open terran conditions. Durst curve shows in axis \\(y\\) the gust factor \\(\\frac{V_t}{V_{3600}}\\), a ration between any wind gust averaged at \\(t\\) seconds, \\(V_t\\), and the hourly averaged wind speed \\(V_{3600}\\), and in the axis \\(x\\) the duration \\(t\\) of the gust. Figure 2.10: Maximun speeds averaged over t (sec), to hourly mean speed "],
["3-rmd-method.html", "Chapter 3 Methodology 3.1 Input Data Selection and Standarization 3.2 Fit data to a POT - Poisson Process 3.3 spatial Interpolation", " Chapter 3 Methodology 3.1 Input Data Selection and Standarization 3.1.1 Data Selection 3.1.2 Data Standarization 3.1.2.1 Anemometer height - 10 m 3.1.2.2 Surface Roughness - 0.03 m 3.1.2.3 Averaging Time - 3-s gust 3.1.3 Data Filterng 3.2 Fit data to a POT - Poisson Process 3.2.1 Data Requirements 3.2.2 Exploratory Data Analysis and Data Preparation 3.2.2.1 Declustering of observations 3.2.2.2 Exclude no-data periods 3.2.2.3 Threshold selection 3.2.3 Parameters Estimation 3.2.3.1 Intensity function 3.2.3.2 Density function 3.2.3.3 Distribution function 3.2.3.4 Maximun likelihood estimation 3.2.4 Velocities at Return Periods 3.3 spatial Interpolation "],
["conclusion.html", "Conclusion", " Conclusion If we don’t want Conclusion to have a chapter number next to it, we can add the {-} attribute. More info And here’s some other random info: the first paragraph after a chapter title or section head shouldn’t be indented, because indents are to tell the reader that you’re starting a new paragraph. Since that’s obvious after a chapter or section title, proper typesetting doesn’t add an indent there. If you feel it necessary to include an appendix, it goes here. --> "],
["A-r-code.html", "A R Code", " A R Code This first appendix includes all of the R chunks of code that were hidden throughout the document (using the include = FALSE chunk tag) to help with readibility and/or setup. In the main Rmd file # This chunk ensures that the thesisdown package is # installed and loaded. This thesisdown package includes # the template files for the thesis. if(!require(devtools)) install.packages(&quot;devtools&quot;, repos = &quot;http://cran.rstudio.com&quot;) if(!require(thesisdown)) devtools::install_github(&quot;ismayc/thesisdown&quot;) library(thesisdown) In Chapter 3: # This chunk ensures that the thesisdown package is # installed and loaded. This thesisdown package includes # the template files for the thesis and also two functions # used for labeling and referencing if(!require(devtools)) install.packages(&quot;devtools&quot;, repos = &quot;http://cran.rstudio.com&quot;) if(!require(dplyr)) install.packages(&quot;dplyr&quot;, repos = &quot;http://cran.rstudio.com&quot;) if(!require(ggplot2)) install.packages(&quot;ggplot2&quot;, repos = &quot;http://cran.rstudio.com&quot;) if(!require(ggplot2)) install.packages(&quot;bookdown&quot;, repos = &quot;http://cran.rstudio.com&quot;) if(!require(thesisdown)){ library(devtools) devtools::install_github(&quot;ismayc/thesisdown&quot;) } library(thesisdown) flights &lt;- read.csv(&quot;data/flights.csv&quot;) "],
["B-the-second-appendix.html", "B The Second Appendix", " B The Second Appendix "],
["references.html", "References", " References "]
]

---
output:
  html_document: default
  pdf_document: default
---
# Theoretical Framework {#rmd-thefra}

<!-- Required to number equations in HTML files -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

```{r include=FALSE}
library("RcmdrMisc")
```

<!--
```{r wrap-hook1, include=FALSE}
library(knitr)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```
-->

## Probability Concepts

Poisson process is an stochastic method that relies in the concepts of probability distributions. The main functions related to probability for extreme value analysis will be described below.

### Probability Density Function - _pdf_

_Pdf_ defines the probability that a continuous variable falls between two points, this is, in _pdf_ the probability is related to the area below the curve (integral) between two points, as for continuous probability distributions the probability at a single point is zero. The term density is directly related to the probability of a portion of the curve, if the density function has high values the probability will be greater in comparison with the same portion of curve for low values.

$$
\int_a^b f(x)dx = Pr[a \leq X \leq b]
$$

Equation \@ref(eq:gumbelpdf) is the Gumbel _pdf_.

<!--
\begin{equation}
  \mathrm{
    f(x)=\frac{1}{\beta}
    e^{\left(-\frac{x-\mu}{\beta}\right)}
    e^{\left(
            -e^{\left(\frac{x-\mu}{\beta}\right)}
      \right)}
  }
  (\#eq:gumbelpdf)
\end{equation}
-->

\begin{equation}
  \mathrm{
          f(x)=\frac{1}{\beta}
          \exp\left\{
            -\frac{x-\mu}{\beta}
          \right\}
          \exp\left\{
            -\exp\left\{
              -\left(
                \frac{x-\mu}{\beta}
              \right)
            \right\}
          \right\},
          \quad -\infty < x < \infty
         }
  (\#eq:gumbelpdf)
\end{equation}

where $\exp\left\{.\right\} \mapsto \mathrm{e}^{\left\{.\right\}}$, $\beta$ is the scale parameter, and $\mu$ is the location parameter. Location ($\mu$) has the effect to shift the _pdf_ to left or right along 'x' axis, thus, if location value is changed the effect is a movement of _pdf_ to the left (small value for location), or to the right (big value for location). Scale has the effect to stretch ($\beta > 1$) of compress  ($0 < \beta< 1$) the _pdf_, if scale parameter is close to zero the _pdf_ approaches a spike.


Figure \@ref(fig:plotgumbelpdffunction) shows _pdf_ with location ($\mu$) = 100 and scale ($\beta$) = 40, using equation \@ref(eq:gumbelpdf). 

```{r plotgumbelpdffunction, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Gumbel pdf", size="footnotesize", fig.width= 4, fig.height= 3}
par(mar=c(2.5,2.5,2,0))
par(oma=c(0,0,0,0))
par(mgp=c(1.5,0.5,0))
location = 100
scale = 40
.x <- seq(0, 300, length.out=1000)
pdfG <- function(x) {
  1/location *exp(-(x-location)/scale)*exp(-exp(-(x-location)/scale))
  }
.y = pdfG(.x)
plot(.x, .y, col="green", lty=4, 
     xlab="Velocities Km/h", ylab="Density Function - Gumbel Distribution", 
     main=paste("Gumbel - Density Function Gumbel Distribution\n", "Location=", 
     round(location,2), " Scale=", round(scale,2)), type="l", 
     cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6)
```

Figure \@ref(fig:plotgumbelpdf) shows _pdf_ with location ($\mu$) = 100 and scale ($\beta$) = 40, using function `dgumbel` of the package `RcmdrMisc`

```{r plotgumbelpdf, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Gumbel pdf - dgumbel function", size="footnotesize", fig.width= 4, fig.height= 3}
par(mar=c(2.5,2.5,2,0))
par(oma=c(0,0,0,0))
par(mgp=c(1.5,0.5,0))
location = 100
scale = 40
.x <- seq(0, 300, length.out=1000)
dfG = dgumbel(.x, location=location, scale=scale)
plot(.x, dfG, col="red", lty=4, 
     xlab="Velocities Km/h", ylab="Density Function - Gumbel Distribution", 
     main=paste("Gumbel - Density Function Gumbel Distribution\n", "Location=", 
     round(location,2), " Scale=", round(scale,2)), type="l", 
     cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6)
```

### Cumulative Distribution Function - _cdf_

_Cdf_ is the probability of taking a value less than or equal to x. That is

$$
F(x) = Pr[X < x] = \alpha
$$
For a continuous variable, _cdf_ can be expressed as the integral of its _pdf_.
$$
F(x) = \int_{-\infty}^x f(x)dx
$$

Equation \@ref(eq:gumbelcdf) is the Gumbel _cdf_.

\begin{equation}
\mathrm{
        F(x) = \exp\left\{-\exp\left[-\left(\frac{x-\mu}{\beta}\right)\right]\right\}, 
        \quad -\infty < x < \infty
        }
  (\#eq:gumbelcdf)
\end{equation}


Figure \@ref(fig:plotgumbelcdffunction) shows Gumbel _cdf_ with location ($\mu$) = 100 and scale ($\beta$) = 40, using equation \@ref(eq:gumbelcdf). As previously done with _pdf_, similar result can be achieved using function `pgumbel` of package `RcmdrMisc`.

```{r plotgumbelcdffunction, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Gumbel cdf", size="footnotesize", fig.width= 4, fig.height= 3}
par(mar=c(2.5,2.5,2,0))
par(oma=c(0,0,0,0))
par(mgp=c(1.5,0.5,0))
location = 100
scale = 40
.x <- seq(0, 300, length.out=1000)
cdfG <- function(x) {
  exp(-exp(-(x-location)/scale))
  }
.y = cdfG(.x)
plot(.x, .y, col="green", lty=4, 
     xlab="Velocities Km/h", ylab="Probability", 
     main=paste("Gumbel - Cumulative Distribution Function\n", "Location=", 
     round(location,2), " Scale=", round(scale,2)), type="l", 
     cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6)
```

### Percent Point Function - _ppf_

_Ppf_ is the inverse of _cdf_, also called the _quantile_ function. This is, from a specific probability get the corresponding value x of the variable.

$$
x = G(\alpha) = G(F(x))
$$
Equation \@ref(eq:gumbelppf) is the Gumbel _ppf_.

\begin{equation}
\mathrm{
        G(\alpha) = \mu-\beta ln(-ln(\alpha))
        \quad 0 < \alpha < 1
        }
  (\#eq:gumbelppf)
\end{equation}


Figure \@ref(fig:plotgumbelppffunction) shows Gumbel _ppf_, using equation \@ref(eq:gumbelppf). Similar result can be achieved using function `qgumbel` of package `RcmdrMisc`.

```{r plotgumbelppffunction, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Gumbel ppf", size="footnotesize", fig.width= 4, fig.height= 3}
par(mar=c(2.5,2.5,2,0))
par(oma=c(0,0,0,0))
par(mgp=c(1.5,0.5,0))
location = 100
scale = 40
.x <- seq(0, 1, length.out=1000)
ppfG <- function(x) {
  location - (scale*log(-log(x)))
  }
.y = ppfG(.x)
plot(.x, .y, col="green", lty=4, 
     ylab="Velocities Km/h", xlab="Probability", 
     main=paste("Gumbel - Percent Point Function\n", "Location=", 
     round(location,2), " Scale=", round(scale,2)), type="l", 
     cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6)
```


### Hazard Function - _hf_

Using $S(x) = 1 - F(x)$ as survival function -_sf_, the probability that a variable takes a value greater than x $S(x) = Pr[X > x] = 1 - F(x)$, the _hf_ is the ratio between _pdf_ and _sf_.

$$
h(x) = \frac{f(x)}{S(x)} = \frac{f(x)}{1-F(x)}
$$
Equation \@ref(eq:gumbelhf) is the Gumbel _ppf_.


\begin{equation}
\mathrm{
        h(x)= \frac{1}{\beta}\frac{\exp(-(x-\mu)/\beta)}{\exp(\exp(-(x-\mu)/\beta))-1}
       }
  (\#eq:gumbelhf)
\end{equation}


Figure \@ref(fig:plotgumbelhffunction) shows Gumbel _hf_, using equation \@ref(eq:gumbelhf).

```{r plotgumbelhffunction, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Gumbel hf", size="footnotesize", fig.width= 4, fig.height= 3}
par(mar=c(2.5,2.5,2,0))
par(oma=c(0,0,0,0))
par(mgp=c(1.5,0.5,0))
location = 100
scale = 40
.x <- seq(0, 1500, length.out=1000)
hfG <- function(x) {
  (1/scale)*(exp(-(x-location)/scale))/(exp(exp(-(x-location)/scale))-1)
  }
.y = hfG(.x)
plot(.x, .y, col="green", lty=4, 
     xlab="Velocities Km/h", ylab="Hazard", 
     main=paste("Gumbel - Hazard Function\n", "Location=", 
     round(location,2), " Scale=", round(scale,2)), type="l", 
     cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6, 
     xlim=c(0,500))
#library(reliaR)
#plot(.x, hgumbel(.x, mu=location, sigma=scale))
#plot(.x, hra.gumbel(.x, mu=location, sigma=scale))
```

## Statistical Concepts For Extreme Analysis

In order to approach the extreme value analysis, some statistical concepts are needed to understand the theoretical framework behind this knowledge area. In this section will be introduced the concepts annual exceedance probability, mean recurrence interval - MRI, exposure time, and compound probability for any given exposure time and MRI.

As an hypothetical example, a simulated database of extreme wind speed will be used. This database is supposed to have 10.000 years of simulated wind speeds.

### Annual Exceedance Probability - $P_e$

Using the previously described database, a question arises to calculate the probability to exceed the highest probable loss due to the simulated winds. It is possible to conclude that there is only one event grater or equal (in this case equal) to the highest probable causing loss in 10.000 years, and it is the _highest wind_. If we sort the database by wind magnitude in descending order (small winds last), the question is solved calculating the annual exceedance probability _Pe_ with next formula 


```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Sorted Winds by Magnitude - wind simulation database", size="footnotesize", fig.width= 3, fig.height= 1}
par(oma=c(0,0,0,0))
par(mar=c(1.5,0,0,0))
par(xpd=NA)
#par(pin = c(3, 1))
plot(1, type="n", xlab="Event Index", ylab="Wind Speed", 
     xlim=c(0,200), ylim= c(0,50), xaxt ="n", yaxt="n", bty="n", asp = 0.8)
x1 = seq(from=10, to=100, by=10)
y1 = c(45, 40, 30, 25, 20, 19, 15, 14, 13, 12)
text(x=x1,  par("usr")[3], 
     labels = x1/10, srt = 0, pos = 1, xpd = TRUE, cex=0.8)
for (i in 1:10) lines(x=c(x1[i],x1[i]), y=c(0,y1[i]))
lines(x=c(125,125), y=c(0,9))
text(x=125,  par("usr")[3], 
     labels = "..." , srt = 0, pos = 1, xpd = TRUE)
x1 = seq(from=150, to=200, by=10)
y1 = c(8, 7, 6, 5, 4, 3)
for (i in 1:10) lines(x=c(x1[i],x1[i]), y=c(0,y1[i]))
text(x=x1,  par("usr")[3], 
     labels = "." , srt = 0, pos = 1, xpd = TRUE, cex=0.8)
axis(1, labels=FALSE, tick=TRUE, col.axis="black", lwd.ticks=0)
#box(which = "outer", col="black", lwd=0.1)
```

$$
P_e = \frac{Event\,index\,after\,descending\,sorting}{Years\,of\,simulations } = \frac{1}{10.000}=0.001=0.01\%
$$
because the highest wind will be the first in the sorted list. Same exercise can be done with all winds to construct the annual exceedance probability curve, that in this case will represent the probability to equal or exceed different probable losses due to wind.

### Return Period - Mean Recurrence Interval - MRI

Continuing with the previous section, if the inverse of the exceedance probability is taken, the return period (in years) is obtained. The return period or Mean Recurrence Interval - MRI is associated with an specific return level (wind extreme velocity). MRI is the numbers of years (N) needed to obtain 63% of change that the corresponding return level will occur at least one time in that period. The return level is expected to be exceeded on average once every N-years. The annual exceedance probability of the return level corresponding to N-years of MRI, is $P_e=\frac{1}{MRI}=\frac{1}{N}$.

For an specific wind extreme event A, the probability that the event will occur in a period equal to MRI years is 63%. If we analyze for the same period a strongest wind extreme event B, its occurrence probability will be lest than 67%. If the purpose of this research is to design infrastructure considering wind loads, the structure will be more resistant to wind if we design with stronger winds, this is high MRIs, and low annual exceedance probability. Common approach for infrastructure design, considering any type of load (earthquake, wind, etc) is to choose high MRI according to the importance/use/risk/type of the structure. For highly important structures, like hospitals or coliseums, where the risk of collapse must be diminished, the MRI used to design is higher in comparison to common structures (for instance a normal house), which implies less risks for its use and importance.

$$
  \mathrm{
    P_e = 
    \begin{cases}
      \begin{split}
            &1-\exp\left(-\frac{1}{MRI}\right),\;for\,MRI\,<\,10\,years
            \\
            &\frac{1}{MRI},\;for\,MRI\,\geq\,10\,years      
      \end{split}
    \end{cases}
  }
$$

### Compound Exceedance Probability - Pn

If time of exposure is consider, understood as time the structure will be in use, it is possible to have a compound probability $P_n$, where $n$ is the exposure period. $P_n$ is the probability that the extreme wind speed will be equaled or exceeded at least one time in $n$ years, and is related with the occurrence probability, but also is possible to calculate the non-occurrence compound probability (probability that the event will not occur).

$$
  \mathrm{
    P_n = 
    \begin{cases}
      \begin{split}
            &1-\left(1-\frac{1}{MRI}\right)^n,\;occurrence\,probability
            \\
            &\left(1-\frac{1}{MRI}\right)^n,\;non-occurrence\,probability
      \end{split}
    \end{cases}
  }
$$

```{r compoundprobability, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Compound Probability", size="footnotesize", fig.width= 5}
par(mar=c(3,3,0,0))
par(oma=c(0,0,0,0))
par(mgp=c(2,1,0))
plot(1, type="n", xlab=expression(paste("Compound Probability ", P[n])), ylab="Exposure Time as a Multiple of MRI", 
     xlim=c(0,1), ylim= c(0,2500), xaxt ="n", yaxt="n", bty="n", cex.lab=0.7)

y1 = c(0, 500,1000,1500,2000,2500)
text(y=y1,  x=par("usr")[1], 
     labels = y1/500, srt = 0, pos = 2, xpd = TRUE, cex=0.8)
y1 = 500*0.69
text(y=y1,  x=par("usr")[1], 
     labels = ".69", srt = 0, pos = 2, xpd = TRUE, cex=0.6)
y1 = 2250
text(y=y1,  x=par("usr")[1], 
     labels = expression(paste("4",frac(1,2))), srt = 0, pos = 2, xpd = TRUE, cex=0.6)

npn <- function(x) (1-(1/500))^x #Event will not occur
n = seq(from=0, to=2500, by=1)
mynpn = npn(n)

lines(x=mynpn,y=n, col= "blue")

pn <- function(x) 1-(1-(1/500))^x #Event will occur
mypn= pn(n)
lines(x=mypn,y=n, col= "green")

text(x=c(0.01, 0.37,0.63, 0.99),  par("usr")[3], 
     labels = c(".01",".37",".63",".99") , srt = 0, pos = 1, xpd = TRUE, cex=0.6)

axis(1, at= seq(from=0, to=1, by= 0.1), labels=seq(from=0, to=1, by= 0.1), tick=TRUE, col.axis="black", cex=0.8)

axis(2, at=c(0, 500,1000,1500,2000,2500),labels=FALSE, tick=TRUE, col.axis="black")
axis(2, at=c(345, 2250),labels=FALSE, tick=TRUE, col.axis="black", tck=-0.015)
axis(1, at=c(0.01,.37,.63,0.99),labels=FALSE, tick=TRUE, col.axis="black", tck=-0.015)

abline(v=c(0.01, 0.37,0.5, 0.63,0.99), lty="dotted")
abline(h=c(345,500, 2250), lty="dotted")
text(x=0.15,  y=1800, 
     labels = "chance event\nwill not occur", cex=0.7)
text(x=0.85,  y=1800, 
     labels = "chance event\nwill occur", cex=0.7)
# box(which = "plot", col="red")
# box(which = "figure", col="blue")
# box(which = "inner", col="cyan")
# box(which = "outer", col="orange")
```

If it is consider exposure time as a multiple of return period, the resulting figure  \@ref(fig:compoundprobability), shows that:

* When exposure time is .69% of the return period, then probability (occurrence and non-occurrence) will be 50%
* As was stated previously, when exposure time is equal to return period, then the probability that the extreme wind speed (return level) occur is 63%, and 37% for the non occurrence probability.
* If exposure time is 4.5 times the return period, there is a 99% of change that the return level will occur.

The example discussed here was presented as an instrument to introduce important concepts, nonetheless,there are specialized approaches to deal with extreme value analysis which will be discussed in [Extreme Value Analysis Overview](#extremeoverview) and more in detail in [Peaks Over Threshold - Poisson Process](#pot-pp). In summary, is necessary to fit the data over a specific threshold to an extreme value distribution, and $P_e$ will be $1-F(y)$, with F(y) as the _cdf_, and MRI as $\frac{1}{1-F(y)}$.


## Extreme Value Analysis Overview {#extremeoverview}

Analysis of extreme values is related with statistical inference to calculate probabilities of extreme events. Main methods to analyze extreme data are epochal, Peaks Over Threshold - POT, and extreme index. The epochal method, also known as block maxima, uses the most extreme value for a specific frame of time, typically, one year. POT is based in the selection of a single threshold value to do the analysis only with values above the threshold. But there are different POT approaches, the most common one is Generalized Pareto Distribution - POT-GPD, but also it is possible to use the Poisson process approach.

In both methods (Epochal and POT), the first step is to fit the data to an appropriate probability distribution model, among them the most used are, - Extreme Value Type I (Gumbel), Extreme Value Type II (FrÃ©chet), Weibull, Generalized Pareto - GPD, and Generalized Extreme Value - GEV.

Distribution models are fitted based in the estimation of its parameters, commonly called location, scale and shape, nonetheless each model has its own parameters names. There are different methods to estimate parameters, among them, - method of moments (modified moments - see @Kubler1994, and L moments - see @Hosking1997), - method of maximum likelihood MLE, see @Harris1994, which is problematic for GPD and GEV, - probability plot correlation coefficient, and - elemental percentiles (for GPD and GEV)

Once candidate parameters are available, it is necessary to assess the goodness of fit of the selected model, using one of the next methods, - Kolmogorov-Smirnov (KS) goodness of fit test, and - Anderson-Darling goodness of fit test. Here a visual assessment is also useful using a probability plot or a kernel density plot with  the fitted _pdf_ overlaid.

The main use of the fitted model is the estimation of mean return intervals - MRI, and extreme wind speeds (return levels),

$$
MRI=\frac{1}{1-F(y)}
$$
with $F(y)$ as the _cdf_. If $1-F(y)$ is the annual exceedance probability, MRI is its inverse, see @Simiu1996 for more details about MRI. If $y$ is solved from previous equation using a given MRI of N-years, its value represents the $Y_N$ wind speed return level,

$$
Y_N = G\left(1-\frac{1}{\lambda\,N}\right)
$$
where $G$ is the _ppf_ (quantile function) and $\lambda$ is the number of wind speeds over the threshold per year.

The CRAN Task View "Extreme Value Analysis" <https://cran.r-project.org/web/views/ExtremeValue.html> shows available **R** for block maxima, POT by GPD, and external indexes estimation approaches. Most important to consider are `evd`, `extremes`, `evir`, `POT`, `extremeStat`, `ismev`, and `Renext`.

### POT-GPD {#pot-gpd}

Short description of POT-GPD (this section need to be complemented)

In POT-GPD, to calculate return levels (RL), $Y_N$, corresponding to the N-years return period, next equation is used,

$$
Y_N =G\left(y, 1-\frac{1}{\lambda\,N}\right)
$$

Where $G$ is the quantile function (_ppf_), and the value of the probability passed to the $G$ function, has to be modified with the $\lambda$ parameter. $\lambda$ is the number of wind speed events over the threshold per year.

 
## Peaks Over Threshold Poisson Process POT-PP {#pot-pp}

According to @Pintar2015 the stochastic Poisson Process - PP is mainly defined by its intensity function. As the intensity function is not uniform over the domain, the PP considered here is non-homogeneous, and due to the intensity function dependency of magnitude and time, it is also bi-dimensional. PP was described for the first time in @Pickands1971, then extended in @Smith1989.

\begin{equation}
  \mathrm{
    \lambda\left(y,t\right)
    \begin{cases}
      \begin{split}
            &\lambda_t(y),\;for\,t\,in\,thunderstorm\,period
            \\
            &\lambda_nt(y),\;for\,t\,in\,non-thunderstorm\,period      
      \end{split}
    \end{cases}
  }
  (\#eq:ppgenericintensityfunction)
\end{equation}

Generic equation \@ref(eq:ppgenericintensityfunction) shows the intensity function, which is defined in the domain $D = D_t\,{\cup}\,D_{nt}$, and allow to fit the PP at each station to the observed data $\{t_i, y_i\}_{i=1}^I$, for all the times ($t_i$) of threshold crossing observations, and its corresponding wind speeds magnitudes ($y_i$). Thus, only data above the threshold (POT) are used.

Intensity function of the PP is defined in @Smith2004, 

\begin{equation}
  \mathrm{
    \frac{1}{\psi_t}\left(1+\zeta_t\frac{y-\omega_t}{\psi_t}\right)_+^{-\frac{1}{\zeta_t}-1}
  }
  (\#eq:ppintensityfunction)
\end{equation}  
        
Where, at a given time $t$, parameter $shape = \zeta_t$ controls the tail length of the intensity function, and the other two parameters $\omega_t$ and $\psi_t$ define the location and scale of the intensity function.


```{r plotdomainpp, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Domain off the Poisson Process - PP", fig.height= 2, fig.width=3.5}
par(mar=c(3,3,0,0))
par(oma=c(0,0,0,0))
par(mgp=c(1.5,0.5,0))
plot(1, type="n", xlab="Time", ylab="Wind Speed", 
     xlim=c(0,30), ylim= c(0,30), xaxt ="n", yaxt="n", bty="n", cex.lab=0.8, cex.axis=0.8)
xpos=c(10,15,20,30)
xtick<-c(c(expression(t[1]), expression(t[2]), expression(t[3]), expression(t[4])))
text(x=xpos,  par("usr")[3], 
     labels = xtick, srt = 0, pos = 1, xpd = TRUE, cex=0.8)

ypos=c(3,6,10,30)
ytick<-c(expression(b[nt]), expression(b[t]), expression(y[1]), expression(y[2]))
text(y=ypos,  x=par("usr")[1], 
     labels = ytick, srt = 0, pos = 2, xpd = TRUE, cex=0.8)

rect(10,10,30,30, density = 5, col= NA, border="black")


lines(x=c(10,10), y=c(0,10), lty="dashed", col = "gray")
lines(x=c(30,30), y=c(0,10), lty="dashed", col = "gray")
lines(x=c(0,10), y=c(10,10), lty="dashed", col = "gray")
lines(x=c(0,10), y=c(30,30), lty="dashed", col = "gray")
lines(x=c(15,15), y=c(10,30), col = "black")
lines(x=c(15,15), y=c(0,10),  lty="dashed", col = "gray")

lines(x=c(20,20), y=c(10,30),  col = "black")
lines(x=c(20,20), y=c(0,10), lty="dashed", col = "gray")

lines(x=c(10,15), y=c(3,3), col = "black")
lines(x=c(20,30), y=c(3,3), col = "black")
lines(x=c(0,10), y=c(3,3), lty="dashed", col = "gray")
lines(x=c(15,20), y=c(3,3), lty="dashed", col = "gray")

lines(x=c(15,20), y=c(6,6), col = "black")
lines(x=c(20,30), y=c(6,6),  lty="dashed", col = "gray")
lines(x=c(0,15), y=c(6,6), lty="dashed", col = "gray")

rect(15,10,20,30, density = 5, angle=-45, col= NA, border="black")
#x <- "\xD0"
#Encoding(x) <- "latin1"
text(28, 28, "D", cex=1.2)
axis(1, labels=FALSE, tick=TRUE, col.axis="black", lwd.ticks=0)
axis(2, labels=FALSE, tick=TRUE, col.axis="black", lwd.ticks=0)
#box(which = "plot", col="red")
#box(which = "figure", col="blue")
#box(which = "inner", col="cyan")
#box(which = "outer", col="orange")
```

Figure \@ref(fig:plotdomainpp) represent the domain $D$ of PP. In time, the domain represents the station service period from first sample $t_1$ to last sample $t_4$. $D$ is the union of all thunderstorm periods $D_t$ (from $t_2$ to $t_3$), and all non-thunderstorm periods $D_{nt}$ (periods $t_1$ to $t_2$ and $t_3$ to $t_4$). In magnitude, only thunderstorm data above its threshold $b_t$, and only non-thunderstorm data above its threshold $b_{nt}$ are used. 

Thunderstorms and non-thunderstorms are modeled independently: 

1. Observations in domain $D$ follow a Poisson distribution with mean $\int_D\lambda(t,y)\,dt\,dy$
2. For each disjoint sub-domain $D_1$ or $D_2$ inside $D$, the observations in $D_1$ or $D_2$ are independent random variables.

Visual representation of the intensity function for PP can be seen in figure \@ref(fig:plotdomain3dpp). In vertical axis, two surfaces were drawn representing independent intensity functions for thunderstorm $\lambda_t(y)$ and for non-thunderstorm $\lambda_{nt}(y)$. The volume under each surface for its corresponding time periods and peak (over threshold) velocities, is the mean of PP.


```{r plotdomain3dpp, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Volume under surfaces represents the mean of PP", dpi=200, fig.align='center'}
#, dpi=300, size="footnotesize", fig.width= 5, fig.height= 5}
include_graphics(path = "figure/domain3d.PNG")
```

To fit the intensity function to the data, the method of maximum likelihood is used to estimate its parameters, $scale = \psi$, $location = \omega$, and $shape = \zeta$, the selected vector of parameters $\eta$ are the $\hat\eta = (\hat\psi, \hat\omega, \hat\zeta)$ values that maximizes next function

\begin{equation}
  \mathrm{
          L(\eta)=\left( 
                        \prod_{i=1}^I\lambda\left(y_i,t_i\right)  
                  \right)
                  \exp\left\{
                            -\int_{{D}}\lambda\left(y,t\right)dy\,dt
                     \right\}
         }
  (\#eq:pplikelihood)
\end{equation}

The values of $\hat\eta$ need to be calculated using a numerical approach, because there is not analytical solution available.

Once the PP is fitted to the data, the model will provide extreme wind velocities (return levels), for different return periods (mean recurrence intervals). 

A $Y_N$ extreme wind velocity, called the return level (RL) belonging to the N-years return period, has a expected frequency to occur or to be exceeded (annual exceedance probability)$P_e = \frac{1}{N}$, and also has a probability that the event does not occur (annual non-exceedance probability) $P_{ne}=1-\frac{1}{N}$. $Y_N$ will be the resulting value of the $G$ (ppf or quantile) function using a probability equal to $P_{ne}$. $Y_N=quantile(y, p=P_{ne})=G(y,p=P_{ne})=ppf(y,p=P_{ne})$. $Y_N$ can be understood as the wind extreme value expected to be exceeded on average once every N years.
 

For PP $Y_N$ is the solution to the next equation, which is defined in terms of the intensity function,

\begin{equation}
  \mathrm{
          \int_{Y_N}^{\infty}\int_0^1\lambda\left( y,t\right)dydt = A_t\int_{Y_N}^{\infty}\lambda_t\left( y\right)\,dy + A_{nt}\int_{Y_N}^{\infty}\lambda_{nt}\left( y\right)\,dy = \frac{1}{N}
         }
  (\#eq:pprl)
\end{equation}

where $A_t$, is the multiplication of the average number of thunderstorm per year and the average length of a thunderstorm, taken to be 1 hour as defined in @Pintar2015, and $A_{nt} = 365 - A_t$. The average length of a non-thunderstorm event is variable, and it is adjusted for each station to guarantee that $A_{nt} + A_t = 365$. Value 365 is used only, if operations with time in the dataset are performed in days.

The same thunderstorm event in considered to occur if the time lag distance between successive thunderstorm samples is small than six hours, and for non-thunderstorm this time is 4 days. For PP, all the measurements belonging to the same event (thunderstorm or non-thunderstorm), need to be de-clustered to leave only one maximum value. In other words, the number of thunderstorm in the time series is one plus the number of time lag distances grater than 6 hours, and for non-thunderstorm grater than 4 days.

### Threshold Selection {#thresholdselection}

POT-PP needs selection of the best threshold pairs $b_t$ and $b_{nt}$ (see figure \@ref(fig:plotdomainpp)) that produces the optimal fit. Measurement of this threshold fitting is done through $W$ statistics. If wind variable $Y$, in a POT-PP approach, has a $cdf = U = F(Y)$, then $F(Y)$ is distributed as Uniform between 0 and 1 - Uniform(0,1), meaning that the transformation $W = -log(1-U)$ is an exponential random variable with mean one (1).

\begin{equation}
  \mathrm{
            cdf = U= F(Y) = P(Y \leq y) = \frac{\int_b^y\lambda(y,t)\,dy}{\int_b^\infty\lambda(y,t)\,dy} 
         }
  (\#eq:ppcdf)
\end{equation}

The procedure to choose the best thresholds pairs based in W transformation, is described in methodology, section [thresholding](#thresholding).



## Wind Loads Requirements {#windloadsrequirements}

As the output maps of this research will be used as input loads for infrastructure design, the methodology used for its creation, need to be consistent with Colombian official wind loads requirements. Colombian structure design code, from now the design standard, was created and it maintained by the Colombian Association of Seismic Engineering - AIS. 

The design standard is mainly based in _minimum design loads and associated criteria for buildings and other structures - ASCE7-16_ norm, see @Asce2017. Under these circumstances, ASCE7-16 defines the minimum requirements of the research products. Especially the chapter C26 - "wind loads - general requirements", C26.5 "wind hazard map", and C26.7 "Exposure" - pages 733 to 747. Wind speeds requirements of ASCE7-16 are based in the combination of independent non-hurricane analysis, and hurricane wind speeds simulations models. The focus of this research will be the analysis of non-hurricane wind data, however, existing results of hurricane studies will be used to present final maps with both components. In ASCE7-16, for non-hurricane wind speed, the procedure is mainly based on @Pintar2015.

ASCE7-16 (page 734), requires the calculation of wind extreme return levels for specific return periods according to the risk category of the structure to be designed: risk category I - 300 years,  risk category II - 700 years,  risk category III - 1700 years,  risk category IV - 3000 years. The design standard only requires 700, 1700 and 3000 years. In addition, extreme wind speeds for those MRI need to correspond to: - 3 second gust speeds, - at 33 ft (10 meters) above the ground, and - exposure category C (open space).

* Risk IV - This are 'indispensable buildings' that involve substantial risk. These structures that can handle toxic or explosive substances.
* Risk III - There is substantial risk because these structures that can handle toxic or explosive substances, can cause a serious economical impact, or massive interruption of activities if they fail.
* Risk II - Category 'by default', and correspond to structures not classified in others categories.
* Risk I - This structures represent low risk for life of people.

To standardize wind speeds to gust speeds ASCE7-16 proposes the curve Durst (see @Durst1960, and figure \@ref(fig:durstcurve)). Durst curve is only valid for open terrain conditions, and it shows in axis $y$ the gust factor $\frac{V_t}{V_{3600}}$, a ratio between any wind gust averaged at $t$ seconds, $V_t$, and the hourly averaged wind speed $V_{3600}$, and in the axis $x$ the duration $t$ of the gust in seconds.


```{r durstcurve, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Maximum speeds averaged over t (sec), to hourly mean speed. Note: curve values taken visually from the original (use original curve for calculations!) ", fig.height=2.5, fig.width=5}
#include_graphics(path = "figure/durst.PNG")
par(mar=c(3,4,0,1))
par(oma=c(0,0,0,0))
par(mgp=c(2,1,0))

#x = c(3800, 1000,  800,   300, 200,   100,  35,  15,  10,   6,      4,    1)
#y = c(1,    1.04, 1.05, 1.10, 1.135, 1.19, 1.3, 1.4, 1.44, 1.475, 1.505, 1.56)
x = c(3800, 2000, 1000, 900, 800, 400, 200, 160, 100, 90, 60, 24, 15, 10, 8, 6, 4.5, 3, 2, 1.7, 1)
y = c(1, 1.02, 1.043, 1.048, 1.055, 1.085, 1.130, 1.15, 1.190, 1.2, 1.245, 1.35, 1.4, 1.435, 1.45, 1.475, 1.495, 1.52, 1.54, 1.55, 1.57)

plot(x=x, y=y, log="x", xlim=c(1,10000), type="n", ylim=c(1,1.7), tck=1, tcl= -0.5, xaxp=c(1,10000,1), xlab="Gust Duration (Seconds)", ylab="", cex.axis=0.7, cex.lab=0.8)
#par(las=1) #Horizontal
corners=par("usr")
text(x=0.2, y=mean(corners[3:4]),  labels=expression(frac(V[t], V[3600])), xpd=TRUE, srt=0, cex=0.8)

#mtext(text = expression(frac(V[t], V[3600])), side=2, line = 2, cex=0.8)

lh <- seq(1, 1.7, by=0.05)
abline(h=lh,col='lightgray', lwd=0.5)
lineasverticaleslogaritmicas <- function(posiciones)
{
    contador <- 0
    xactual <- 0
    posiciones <- posiciones
    for(.x in axTicks(1)) {
        xanterior <- xactual
        xactual <- .x
        if (contador > 0)
        {
         paso <- (xactual-xanterior)/9
         a1 <- seq(xanterior, xactual, by=paso)
         abline(v=a1[posiciones], col='lightgray', lwd=0.5)
         #rug (a1, col='red', ticksize=0.03)
         #text(a1, par('usr')[3]-0.01, srt=90, adj=1, labels=a1, xpd=TRUE, col='gray50', cex=0.6)
        }
        contador <- contador + 1
    }
}
lineasverticaleslogaritmicas(1:10)

lh <- seq(1.1, 1.7, by=0.1)
abline(h=lh,col='black', lwd=0.8)
lv <- c(1,10,100, 1000, 10000)
abline(v=lv,col='black', lwd=0.8)

smoothingSpline = smooth.spline(x, y, spar=0.35)
lines(smoothingSpline, col="red", lwd=2)

legend('topright', c('Synoptic (Durst, 1960)'), lty=c('solid'), lwd=c('2'), col=c('red'), inset = .04, cex=0.7, ncol=1, merge=F, box.col = "black",bg = "white")
#box(which = "plot", col="red")
#box(which = "figure", col="blue")
#box(which = "inner", col="cyan")
#box(which = "outer", col="orange")
```



---
output:
  pdf_document: default
  html_document: default
---
# Theoretical Framework {#rmd-thefra}

<!-- Required to number equations in HTML files -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

```{r include=FALSE}
library("RcmdrMisc")
```

<!--
```{r wrap-hook1, include=FALSE}
library(knitr)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```
-->

## Probability Concepts

Poisson process is an stochastic method that relies in the concepts of probability distributions. The main functions related to probability for extreme value analysis will be described below.

### Probability Density Function - _pdf_

Pdf defines the probability that a continuos variable falls between two points, this is, in _pdf_ the proability is related to the area below the curve (integral) between two points, as for continuos probability distributions the probability at a single point is zero. The term density is directly related to the probability of a portion of the curve, if the density function has high values the probability will be greater in comparison with the same portion of curve for low values.

$$
\int_a^b f(x)dx = Pr[a \leq X \leq b]
$$

Equation \@ref(eq:gumbelpdf) is the Gumbel _pdf_.

<!--
\begin{equation}
  \mathrm{
    f(x)=\frac{1}{\beta}
    e^{\left(-\frac{x-\mu}{\beta}\right)}
    e^{\left(
            -e^{\left(\frac{x-\mu}{\beta}\right)}
      \right)}
  }
  (\#eq:gumbelpdf)
\end{equation}
-->

\begin{equation}
  \mathrm{
          f(x)=\frac{1}{\beta}
          \exp\left\{
            -\frac{x-\mu}{\beta}
          \right\}
          \exp\left\{
            -\exp\left\{
              -\left(
                \frac{x-\mu}{\beta}
              \right)
            \right\}
          \right\},
          \quad -\infty < x < \infty
         }
  (\#eq:gumbelpdf)
\end{equation}

where $\exp\left\{.\right\} \mapsto \mathrm{e}^{\left\{.\right\}}$, $\beta$ is the scale parameter, and $\mu$ is the location parameter. Location ($\mu$) has the effect to shift the _pdf_ to left or right along 'x' axis, thus, if location value is changed the effect is a movement of _pdf_ to the left (small value for location), or to the right (big value for location). Scale has the effect to stretch ($\beta > 1$) of compress  ($0 < \beta< 1$) the _pdf_, if scale parameter is close to zero the pdf approaches a spike.


Figure \@ref(fig:plotgumbelpdffunction) shows _pdf_ with location ($\mu$) = 100 and scale ($\beta$) = 40, using equation \@ref(eq:gumbelpdf). 

```{r plotgumbelpdffunction, echo=T, message=FALSE, warning=FALSE, fig.cap="Gumbel pdf", size="footnotesize", fig.width= 4, fig.height= 3}
location = 100
scale = 40
.x <- seq(0, 300, length.out=1000)
pdfG <- function(x) {
  1/location *exp(-(x-location)/scale)*exp(-exp(-(x-location)/scale))
  }
.y = pdfG(.x)
plot(.x, .y, col="green", lty=4, 
     xlab="Velocities Km/h", ylab="Density Function - Gumbel Distribution", 
     main=paste("Gumbel - Density Function Gumbel Distribution\n", "Location=", 
     round(location,2), " Scale=", round(scale,2)), type="l", 
     cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6)
```

Figure \@ref(fig:plotgumbelpdf) shows _pdf_ with location ($\mu$) = 100 and scale ($\beta$) = 40, using function `dgumbel` of the package `RcmdrMisc`

```{r plotgumbelpdf, echo=T, message=FALSE, warning=FALSE, fig.cap="Gumbel pdf - dgumbel function", size="footnotesize", fig.width= 4, fig.height= 3}
location = 100
scale = 40
.x <- seq(0, 300, length.out=1000)
dfG = dgumbel(.x, location=location, scale=scale)
plot(.x, dfG, col="red", lty=4, 
     xlab="Velocities Km/h", ylab="Density Function - Gumbel Distribution", 
     main=paste("Gumbel - Density Function Gumbel Distribution\n", "Location=", 
     round(location,2), " Scale=", round(scale,2)), type="l", 
     cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6)
```

### Cumulative Distribution Funtcion - _cdf_

_Cdf_ is the probability of taking a value less than or equal to x. That is

$$
F(x) = Pr[X < x] = \alpha
$$
For a continuous variable, _cdf_ can be expressed as the integral of its _pdf_.
$$
F(x) = \int_{-\infty}^x f(x)dx
$$

Equation \@ref(eq:gumbelcdf) is the Gumbel _cdf_.

\begin{equation}
\mathrm{
        F(x) = \exp\left\{-\exp\left[-\left(\frac{x-\mu}{\beta}\right)\right]\right\}, 
        \quad -\infty < x < \infty
        }
  (\#eq:gumbelcdf)
\end{equation}


Figure \@ref(fig:plotgumbelcdffunction) shows Gumbel _cdf_ with location ($\mu$) = 100 and scale ($\beta$) = 40, using equation \@ref(eq:gumbelcdf). As previously done with _pdf_, similar result can be achieved using function `pgumbel` of package `RcmdrMisc`.

```{r plotgumbelcdffunction, echo=T, message=FALSE, warning=FALSE, fig.cap="Gumbel cdf", size="footnotesize", fig.width= 4, fig.height= 3}
location = 100
scale = 40
.x <- seq(0, 300, length.out=1000)
cdfG <- function(x) {
  exp(-exp(-(x-location)/scale))
  }
.y = cdfG(.x)
plot(.x, .y, col="green", lty=4, 
     xlab="Velocities Km/h", ylab="Probability", 
     main=paste("Gumbel - Cumulative Distribution Function\n", "Location=", 
     round(location,2), " Scale=", round(scale,2)), type="l", 
     cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6)
```

### Percent Point Function - _ppf_

_Ppf_ is the inverse of _cdf_, also called the _quantile_ function. This is, from a specific probability get the corresponding value x of the variable.

$$
x = G(\alpha) = G(F(x))
$$
Equation \@ref(eq:gumbelppf) is the Gumbel _ppf_.

\begin{equation}
\mathrm{
        G(\alpha) = \mu-\beta ln(-ln(\alpha))
        \quad 0 < \alpha < 1
        }
  (\#eq:gumbelppf)
\end{equation}


Figure \@ref(fig:plotgumbelppffunction) shows Gumbel _ppf_, using equation \@ref(eq:gumbelppf). Similar result can be achieved using function `qgumbel` of package `RcmdrMisc`.

```{r plotgumbelppffunction, echo=T, message=FALSE, warning=FALSE, fig.cap="Gumbel cdf", size="footnotesize", fig.width= 4, fig.height= 3}
location = 100
scale = 40
.x <- seq(0, 1, length.out=1000)
ppfG <- function(x) {
  location - (scale*log(-log(x)))
  }
.y = ppfG(.x)
plot(.x, .y, col="green", lty=4, 
     ylab="Velocities Km/h", xlab="Probability", 
     main=paste("Gumbel - Percent Point Function\n", "Location=", 
     round(location,2), " Scale=", round(scale,2)), type="l", 
     cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6)
```


### Hazard Function - _hf_

Using $S(x) = 1 - F(x)$ as survival function -_sf_, the probability that a variable takes a value greather than x $S(x) = Pr[X > x] = 1 - F(x)$, the _hf_ is the ratio between _pdf_ and _sf_.

$$
h(x) = \frac{f(x)}{S(x)} = \frac{f(x)}{1-F(x)}
$$
Equation \@ref(eq:gumbelhf) is the Gumbel _ppf_.


\begin{equation}
\mathrm{
        h(x)= \frac{1}{\beta}\frac{\exp(-(x-\mu)/\beta)}{\exp(\exp(-(x-\mu)/\beta))-1}
       }
  (\#eq:gumbelhf)
\end{equation}


Figure \@ref(fig:plotgumbelhffunction) shows Gumbel _hf_, using equation \@ref(eq:gumbelhf).

```{r plotgumbelhffunction, echo=T, message=FALSE, warning=FALSE, fig.cap="Gumbel cdf", size="footnotesize", fig.width= 4, fig.height= 3}
location = 100
scale = 40
.x <- seq(0, 3000, length.out=1000)
hfG <- function(x) {
  (1/scale)*(exp(-(x-location)/scale))/(exp(exp(-(x-location)/scale))-1)
  }
.y = hfG(.x)
plot(.x, .y, col="green", lty=4, 
     xlab="Velocities Km/h", ylab="Hazard", 
     main=paste("Gumbel - Hazard Function\n", "Location=", 
     round(location,2), " Scale=", round(scale,2)), type="l", 
     cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6)
#library(reliaR)
#plot(.x, hgumbel(.x, mu=location, sigma=scale))
#plot(.x, hra.gumbel(.x, mu=location, sigma=scale))
```

## Introductory concepts for statistical analysis of extreme values

In order to approach the extreme value analysis, some statistical concepts are needed to understand the theoretical framework behind this knowledge area. In this section will be introduced the concepts annual excedance probability, mean recurrence interval - MRI, exposure time, and compound probability for any given exposure time and MRI.

As an hypotetical example, a simulated database of extreme wind speed will be used. This database is supposed to have 10.000 years of simulated wind speeds.

### Annual Excedance Probability - Pe

Using the previously described database, a question arises to calculate the probability to exceed the highest probable loss due to the simulated winds. It is possible to conclude that there is only one event grather or equal (in this case equal) to the higest probable causing loss in 10.000 years, and it is the _highest wind_. If we sort the database by wind magnitude in descending order (small winds last), the question is solved calculating the annual excedance probability _Pe_ with next formula 


```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Gumbel cdf", size="footnotesize", fig.width= 4, fig.height= 3}
par(pin = c(4, 1))
plot(1, type="n", xlab="Event Index - Ordered Winds by Magnitud", ylab="Wind Speed", 
     xlim=c(0,200), ylim= c(0,50), xaxt ="n", yaxt="n", bty="n", asp = 0.8)
x1 = seq(from=10, to=100, by=10)
y1 = c(45, 40, 30, 25, 20, 19, 15, 14, 13, 12)
text(x=x1,  par("usr")[3], 
     labels = x1/10, srt = 0, pos = 1, xpd = TRUE, cex=0.8)
for (i in 1:10) lines(x=c(x1[i],x1[i]), y=c(0,y1[i]))
lines(x=c(125,125), y=c(0,9))
text(x=125,  par("usr")[3], 
     labels = "..." , srt = 0, pos = 1, xpd = TRUE)
x1 = seq(from=150, to=200, by=10)
y1 = c(8, 7, 6, 5, 4, 3)
for (i in 1:10) lines(x=c(x1[i],x1[i]), y=c(0,y1[i]))
text(x=x1,  par("usr")[3], 
     labels = "." , srt = 0, pos = 1, xpd = TRUE, cex=0.8)
axis(1, labels=FALSE, tick=TRUE, col.axis="black", lwd.ticks=0)
```

$$
P_e = \frac{Event\,index\,after\,descending\,sorting}{Years\,of\,simulations } = \frac{1}{10.000}=0.001=0.01\%
$$
because the highest wind will be the first in the sorted list. Same exercise can be done with all winds to construct the annual exedance probability curve, that in this case will represent the probability to equal or exceed different probable losses due to wind.

### Return Period - Mean Recurrence Interval

Continuing with the previous section, if the inverse of the excedance probability is taken, the return period is obtained. The return period or Mean Recurrence Interval - MRI.


### Compound Excedance Probability - Pn

## Extreme Value Analysis Overview

Analysis of extreme values is related with statistical inference to calculate probabilities of extreme events. Main methods to analize extreme data are ephochal, Peaks Over Threshold - POT, and extreme index. The epochal method, also known as block maxima, uses the most extreme value for a specific frame of time, tipically, one year. POT is based in the selection of a single threshold value to do the analysis only with values above the threshold. But there are different POT aproaches, the most commond one is Generalized Paretto Distribution - POT-GPD, but also it is possible to use the Poisson process approach.

In both methods (Epochal and POT), the first step is to fit the data to an appropiate probability distribution model, among them the most used are, - Extreme Value Type I (Gumbel), Extreme Value Type II (Frechet), Weibull, Generalized Pareto - GPD, and Generalized Extreme Value - GEV.

Distribution models are fitted based in the estimation of its parameters, mommonly called location, scale and shape, nonetheless each model has its own parameters names. There are different methods to estimate parameters, among them, - method of moments (modified moments - see @Kubler1994, and L moments - see @Hosking1997), - method of maximum likelihood MLE, see @Harris1994, which is problematic for GPD and GEV, - probability plot correlation coeficient, and - elemental percentiles (for GPD and GEV)

Once cadidate parameters are available, it is neccesary to assess the goodness of fit of the selected model, using one of the next methods, - Kolmogorov-Smirnov (KS) goodnes of fit test, and - Anderson-Darling goodness of fit test. Here a visual assesment is also useful using a probability plot or a kernel density plot with  the fitted _pdf_ overlaid.

The main use of the fitted model is the estimation of mean return intervals - MRI, and extreme wind speeds (return levels),

$$
MRI=\frac{1}{1-F(y)}
$$
with $F(y)$ as the _cdf_. If $1-F(y)$ is the annual excedance probability, MRI is its inverse, see @Simiu1996 for more details about MRI. If $y$ is solved from previos equation using a given MRI of N-years, its value represents the $Y_N$ wind speed return level,

$$
Y_N = G\left(1-\frac{1}{\lambda\,N}\right)
$$
where $G$ is the _ppf_ (quantile function) and $\lambda$ is the number of wind spees over the threshold per year.

The CRAN Task View "Extreme Value Analysis" <https://cran.r-project.org/web/views/ExtremeValue.html> shows available **R** for block maxima, POT by GPD, and external indexes estimation aproaches. Most important to consider are `evd`, `extremes`, `evir`, `POT`, `extremeStat`, `ismev`, and `Renext`.

 
## Peaks Over Threshold - Poisson Process

According to @Pintar2015 the stochastic poisson process is mainly defined by its intensity function. As the intensity function is nos uniform over the domain, the poisson process considered here is non-homogeneous, and due to the intensity function dependance of magnitud and time, it is also bi-dimmensional. Poisson Process was described for the first time in @Pickands1971, then extended in @Smith1989.

\begin{equation}
  \mathrm{
    \lambda\left(y,t\right)
    \begin{cases}
      \begin{split}
            &\lambda_t(y),\;for\,t\,in\,thunderstorm\,period
            \\
            &\lambda_nt(y),\;for\,t\,in\,non-thunderstorm\,period      
      \end{split}
    \end{cases}
  }
  (\#eq:ppgenericintensityfunction)
\end{equation}

Generic equation \@ref(eq:ppintensityfunction) shows the intensity function, which is defined in the domain $D = D_t\,{\cup}\,D_{nt}$, and allow to fit the poisson process at each station to the observed data $\{t_i, y_i\}_{i=1}^I$ for al the times ($t_i$) of threshold crossing observations and its corresponding wind speeds magnitudes ($y_i$). Thus, only data above the threshold is used.

Intensity function of the Poisson Process is defined in @Smith2004, 

$$
\frac{1}{\psi_t}\left(1+\zeta_t\frac{y-\omega_t}{\psi_t}\right)_+^{-\frac{1}{\zeta_t}-1}
$$
Where $\zeta_t$ controls the tail lengh of the intensity function at a given time $t$, but to facilitate the estimation of the parameters then $\zeta_t$ is taken to be zero, then doing the limit, the resulting intensity function is the same as the the GEV type I or Gumbel distribution,

$$
\frac{1}{\psi_t}\exp\left\{\frac{-(y-\omega_t)}{\psi_t}\right\}
$$

In this study, the used intensity functions are shown in ecuation \@ref(eq:ppspecificintensityfunction).

\begin{equation}
  \mathrm{
    \lambda\left(y,t\right)
    \begin{cases}
      \begin{split}
            &\frac{1}{\psi_s}\exp\left\{\frac{-(y-\omega_s)}{\psi_s}\right\},\;for\,t\,in\,thunderstorm\,period
            \\
            &\frac{1}{\psi_{nt}}\exp\left\{\frac{-(y-\omega_{nt})}{\psi_{nt}}\right\},\;for\,t\,in\,non-thunderstorm\,period      
      \end{split}
    \end{cases}
  }
  (\#eq:ppspecificintensityfunction)
\end{equation}


```{r plotdomainpp, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Domanin off the Poisson Process", size="footnotesize", fig.width= 4, fig.height= 4}
plot(1, type="n", xlab="Time", ylab="Wind Speed", 
     xlim=c(0,30), ylim= c(0,30), xaxt ="n", yaxt="n", bty="n")
xpos=c(10,15,20,30)
xtick<-c(c(expression(t[1]), expression(t[2]), expression(t[3]), expression(t[4])))
text(x=xpos,  par("usr")[3], 
     labels = xtick, srt = 0, pos = 1, xpd = TRUE)

ypos=c(3,6,10,30)
ytick<-c(expression(b[nt]), expression(b[t]), expression(y[1]), expression(y[2]))
text(y=ypos,  x=par("usr")[1], 
     labels = ytick, srt = 0, pos = 2, xpd = TRUE)

rect(10,10,30,30, density = 5, col= NA, border="black")


lines(x=c(10,10), y=c(0,10), lty="dashed", col = "gray")
lines(x=c(30,30), y=c(0,10), lty="dashed", col = "gray")
lines(x=c(0,10), y=c(10,10), lty="dashed", col = "gray")
lines(x=c(0,10), y=c(30,30), lty="dashed", col = "gray")
lines(x=c(15,15), y=c(10,30), col = "black")
lines(x=c(15,15), y=c(0,10),  lty="dashed", col = "gray")

lines(x=c(20,20), y=c(10,30),  col = "black")
lines(x=c(20,20), y=c(0,10), lty="dashed", col = "gray")

lines(x=c(10,15), y=c(3,3), col = "black")
lines(x=c(20,30), y=c(3,3), col = "black")
lines(x=c(0,10), y=c(3,3), lty="dashed", col = "gray")
lines(x=c(15,20), y=c(3,3), lty="dashed", col = "gray")

lines(x=c(15,20), y=c(6,6), col = "black")
lines(x=c(20,30), y=c(6,6),  lty="dashed", col = "gray")
lines(x=c(0,15), y=c(6,6), lty="dashed", col = "gray")

rect(15,10,20,30, density = 5, angle=-45, col= NA, border="black")
#x <- "\xD0"
#Encoding(x) <- "latin1"
text(28, 28, "D", cex=1.2)
axis(1, labels=FALSE, tick=TRUE, col.axis="black", lwd.ticks=0)
axis(2, labels=FALSE, tick=TRUE, col.axis="black", lwd.ticks=0)
```

Figure \@ref(fig:plotdomainpp) represent the domain $D$ of the Poisson process. In time, the domain represents the station service period from first sample $t_1$ to last sample $t_4$. $D$ is the union of all thunderstorm periods $D_t$ (from $t_2$ to $t_3$), and all non-thunderstorm periods $D_{nt}$ (periods $t_1$ to $t_2$ and $t_3$ to $t_4$). In magnitud, only thunderstorm data above its threshold $b_t$, and only non-tunderstorm data above its threshold $b_{nt}$ are used. 

Thunderstoms and non-thunderstorms are modeled independently: 

1. Observations in domain $D$ follow a Poisson distribution with mean $\int_D\lambda(t,y)\,dt\,dy$
2. For each disjoint subdomain $D_1$ or $D_2$ inside $D$, the observations in $D_1$ or $D_2$ are independent random variables.

Visual representation of the intensity function for the Poisson Process can be seen in figure \@ref(fig:plotdomain3dpp). In vertical axis, two surfaces were drawn representing independent intensity functions for thunderstorm $\lambda_t(y)$ and for non-thunderstorm $\lambda_{nt}(y)$. The volume under each surface for its corresponding time periods and peak (over threshold) velocities, is the mean of the Poisson Process.


```{r plotdomain3dpp, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Volume under surfaces represents the mean of the Poisson process", size="footnotesize", fig.width= 5, fig.height= 5}
include_graphics(path = "figure/domain3d.PNG")
```


The method of maximun likelihood es used to estimate the parameters of the Poisson process, the selected vector of parameters $\eta$ are the $\hat\eta$ values that maximizes the function

\begin{equation}
  \mathrm{
          L(\eta)=\left( 
                        \prod_{i=1}^I\lambda\left(y_i,t_i\right)  
                  \right)
                  \exp\left\{
                            -\int_{{D}}\lambda\left(y,t\right)dy\,dt
                     \right\}
         }
  (\#eq:pplikelihood)
\end{equation}

$\hat\eta$ values need to be calculated using a numericall approach because there is not analytical solution available.

Once the Poisson process is fittet to the data, the model will provide extreme wind velocities (return levels), for different return periods (mean recurrence intervals). 


A $Y_N$ extreme wind velocity, called the return level (RL) belonging to the N-years return period, has a expected frequency to occur or to be exceeded (annual excedance probability)$P_e = \frac{1}{N}$, and also has a probability that the event does not occur (annual non-excedance probability) $P_{ne}=1-\frac{1}{N}$. $Y_N$ will be the resulting value of the $G$ (ppf or quantile) function using a probability equal to $P_{ne}$. $Y_N=quantile(y, p=P_{ne})=G(x,p=P_{ne})=ppf(x,p=P_{ne})$. As for this study $\zeta = 0$, the $G$ function to use is the Gumbel quantile function. $Y_N$ can be undestood as the wind extreme value expected to be exceeded on average once every N years.

For different POT approaches, as POT-GPD described --, the value of the probability passed to the $G$ function, has to be modified with the $\lambda$ parameter, as is described in next equation. $\lambda$ is the number of wind speed over the threshold per year.

$$
Y_N =G\left(y, 1-\frac{1}{\lambda\,N}\right)
$$

 

For the Poisson process $Y_N$ is also the solution to the next equation, which is defined in terms of the intensity function,


\begin{equation}
  \mathrm{
          \int_{Y_N}^{\infty}\int_0^1\lambda\left( y,t\right)dydt = A_t\int_{Y_N}^{\infty}\lambda_t\left( y\right)\,dy + A_{nt}\int_{Y_N}^{\infty}\lambda_{nt}\left( y\right)\,dy = \frac{1}{N}
         }
  (\#eq:ppmri)
\end{equation}

where $A_t$, is the multiplication of the average number of thunderstorm per year and the average lengh of a thunderstorm (taken to be 1 hour as defined in @Pintar2015), and $A_{nt} = 1 - A_t$. The average length of a non-thunderstorm event is variable, and it is adjusted in each station to guarantee that $A_{nt} + A_t = 1$

The same thunderstorm event in considered to occur if the time lag distance between sucesive thunderstorm samples is small than six hours, and for non-thunderstorm this time is 4 days. For the Poisson process, all the measurements belonging to the same event (thunderstorm or non tunderstorm), need to be declustered to leave only one maximun value. In other words, the number of thunderstorm in the time serie is the number of time lag distances grather than 6 hours, and for non-thunderstorm grather than 4 days.

###Threshold Selection

$$U=F(Y)$$
$$W = -log(1-U)$$


---
output:
  html_document: default
  pdf_document: default
---
# Theoretical Framework {#rmd-thefra}

<!-- Required to number equations in HTML files -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

```{r include=FALSE}
library("RcmdrMisc")
```

<!--
```{r wrap-hook1, include=FALSE}
library(knitr)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```
-->

## Probability Concepts

Poisson process is an stochastic method that relies in the concepts of probability distributions. The main functions related to probability for extreme value analysis will be described below.

### Probability Density Function - _pdf_

Pdf defines the probability that a continuos variable falls between two points, this is, in _pdf_ the proability is related to the area below the curve (integral) between two points, as for continuos probability distributions the probability at a single point is zero. The term density is directly related to the probability of a portion of the curve, if the density function has high values the probability will be greater in comparison with the same portion of curve for low values.

$$
\int_a^b f(x)dx = Pr[a \leq X \leq b]
$$

Equation \@ref(eq:gumbelpdf) is the Gumbel _pdf_.

<!--
\begin{equation}
  \mathrm{
    f(x)=\frac{1}{\beta}
    e^{\left(-\frac{x-\mu}{\beta}\right)}
    e^{\left(
            -e^{\left(\frac{x-\mu}{\beta}\right)}
      \right)}
  }
  (\#eq:gumbelpdf)
\end{equation}
-->

\begin{equation}
  \mathrm{
          f(x)=\frac{1}{\beta}
          \exp\left\{
            -\frac{x-\mu}{\beta}
          \right\}
          \exp\left\{
            -\exp\left\{
              -\left(
                \frac{x-\mu}{\beta}
              \right)
            \right\}
          \right\},
          \quad -\infty < x < \infty
         }
  (\#eq:gumbelpdf)
\end{equation}

where $\exp\left\{.\right\} \mapsto \mathrm{e}^{\left\{.\right\}}$, $\beta$ is the scale parameter, and $\mu$ is the location parameter. Location ($\mu$) has the effect to shift the _pdf_ to left or right along 'x' axis, thus, if location value is changed the effect is a movement of _pdf_ to the left (small value for location), or to the right (big value for location). Scale has the effect to stretch ($\beta > 1$) of compress  ($0 < \beta< 1$) the _pdf_, if scale parameter is close to zero the pdf approaches a spike.


Figure \@ref(fig:plotgumbelpdffunction) shows _pdf_ with location ($\mu$) = 100 and scale ($\beta$) = 40, using equation \@ref(eq:gumbelpdf). 

```{r plotgumbelpdffunction, echo=T, message=FALSE, warning=FALSE, fig.cap="Gumbel pdf", size="footnotesize", fig.width= 4, fig.height= 3}
location = 100
scale = 40
.x <- seq(0, 300, length.out=1000)
pdfG <- function(x) {
  1/location *exp(-(x-location)/scale)*exp(-exp(-(x-location)/scale))
  }
.y = pdfG(.x)
plot(.x, .y, col="green", lty=4, 
     xlab="Velocities Km/h", ylab="Density Function - Gumbel Distribution", 
     main=paste("Gumbel - Density Function Gumbel Distribution\n", "Location=", 
     round(location,2), " Scale=", round(scale,2)), type="l", 
     cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6)
```

Figure \@ref(fig:plotgumbelpdf) shows _pdf_ with location ($\mu$) = 100 and scale ($\beta$) = 40, using function `dgumbel` of the package `RcmdrMisc`

```{r plotgumbelpdf, echo=T, message=FALSE, warning=FALSE, fig.cap="Gumbel pdf - dgumbel function", size="footnotesize", fig.width= 4, fig.height= 3}
location = 100
scale = 40
.x <- seq(0, 300, length.out=1000)
dfG = dgumbel(.x, location=location, scale=scale)
plot(.x, dfG, col="red", lty=4, 
     xlab="Velocities Km/h", ylab="Density Function - Gumbel Distribution", 
     main=paste("Gumbel - Density Function Gumbel Distribution\n", "Location=", 
     round(location,2), " Scale=", round(scale,2)), type="l", 
     cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6)
```

### Cumulative Distribution Funtcion - _cdf_

_Cdf_ is the probability of taking a value less than or equal to x. That is

$$
F(x) = Pr[X < x] = \alpha
$$
For a continuous variable, _cdf_ can be expressed as the integral of its _pdf_.
$$
F(x) = \int_{-\infty}^x f(x)dx
$$

Equation \@ref(eq:gumbelcdf) is the Gumbel _cdf_.

\begin{equation}
\mathrm{
        F(x) = \exp\left\{-\exp\left[-\left(\frac{x-\mu}{\beta}\right)\right]\right\}, 
        \quad -\infty < x < \infty
        }
  (\#eq:gumbelcdf)
\end{equation}


Figure \@ref(fig:plotgumbelcdffunction) shows Gumbel _cdf_ with location ($\mu$) = 100 and scale ($\beta$) = 40, using equation \@ref(eq:gumbelcdf). As previously done with _pdf_, similar result can be achieved using function `pgumbel` of package `RcmdrMisc`.

```{r plotgumbelcdffunction, echo=T, message=FALSE, warning=FALSE, fig.cap="Gumbel cdf", size="footnotesize", fig.width= 4, fig.height= 3}
location = 100
scale = 40
.x <- seq(0, 300, length.out=1000)
cdfG <- function(x) {
  exp(-exp(-(x-location)/scale))
  }
.y = cdfG(.x)
plot(.x, .y, col="green", lty=4, 
     xlab="Velocities Km/h", ylab="Probability", 
     main=paste("Gumbel - Cumulative Distribution Function\n", "Location=", 
     round(location,2), " Scale=", round(scale,2)), type="l", 
     cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6)
```

### Percent Point Function - _ppf_

_Ppf_ is the inverse of _cdf_, also called the _quantile_ function. This is, from a specific probability get the corresponding value x of the variable.

$$
x = G(\alpha) = G(F(x))
$$
Equation \@ref(eq:gumbelppf) is the Gumbel _ppf_.

\begin{equation}
\mathrm{
        G(\alpha) = \mu-\beta ln(-ln(\alpha))
        \quad 0 < \alpha < 1
        }
  (\#eq:gumbelppf)
\end{equation}


Figure \@ref(fig:plotgumbelppffunction) shows Gumbel _ppf_, using equation \@ref(eq:gumbelppf). Similar result can be achieved using function `qgumbel` of package `RcmdrMisc`.

```{r plotgumbelppffunction, echo=T, message=FALSE, warning=FALSE, fig.cap="Gumbel cdf", size="footnotesize", fig.width= 4, fig.height= 3}
location = 100
scale = 40
.x <- seq(0, 1, length.out=1000)
ppfG <- function(x) {
  location - (scale*log(-log(x)))
  }
.y = ppfG(.x)
plot(.x, .y, col="green", lty=4, 
     ylab="Velocities Km/h", xlab="Probability", 
     main=paste("Gumbel - Percent Point Function\n", "Location=", 
     round(location,2), " Scale=", round(scale,2)), type="l", 
     cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6)
```


### Hazard Function - _hf_

Using $S(x) = 1 - F(x)$ as survival function -_sf_, the probability that a variable takes a value greather than x $S(x) = Pr[X > x] = 1 - F(x)$, the _hf_ is the ratio between _pdf_ and _sf_.

$$
h(x) = \frac{f(x)}{S(x)} = \frac{f(x)}{1-F(x)}
$$
Equation \@ref(eq:gumbelhf) is the Gumbel _ppf_.


\begin{equation}
\mathrm{
        h(x)= \frac{1}{\beta}\frac{\exp(-(x-\mu)/\beta)}{\exp(\exp(-(x-\mu)/\beta))-1}
       }
  (\#eq:gumbelhf)
\end{equation}


Figure \@ref(fig:plotgumbelhffunction) shows Gumbel _hf_, using equation \@ref(eq:gumbelhf).

```{r plotgumbelhffunction, echo=T, message=FALSE, warning=FALSE, fig.cap="Gumbel cdf", size="footnotesize", fig.width= 4, fig.height= 3}
location = 100
scale = 40
.x <- seq(0, 3000, length.out=1000)
hfG <- function(x) {
  (1/scale)*(exp(-(x-location)/scale))/(exp(exp(-(x-location)/scale))-1)
  }
.y = hfG(.x)
plot(.x, .y, col="green", lty=4, 
     xlab="Velocities Km/h", ylab="Hazard", 
     main=paste("Gumbel - Hazard Function\n", "Location=", 
     round(location,2), " Scale=", round(scale,2)), type="l", 
     cex.axis = 0.5, cex.lab= 0.6, cex.main=0.7, cex.sub=0.6)
#library(reliaR)
#plot(.x, hgumbel(.x, mu=location, sigma=scale))
#plot(.x, hra.gumbel(.x, mu=location, sigma=scale))
```

## Statistical Concepts For Extreme Analysis

In order to approach the extreme value analysis, some statistical concepts are needed to understand the theoretical framework behind this knowledge area. In this section will be introduced the concepts annual excedance probability, mean recurrence interval - MRI, exposure time, and compound probability for any given exposure time and MRI.

As an hypotetical example, a simulated database of extreme wind speed will be used. This database is supposed to have 10.000 years of simulated wind speeds.

### Annual Excedance Probability - Pe

Using the previously described database, a question arises to calculate the probability to exceed the highest probable loss due to the simulated winds. It is possible to conclude that there is only one event grather or equal (in this case equal) to the higest probable causing loss in 10.000 years, and it is the _highest wind_. If we sort the database by wind magnitude in descending order (small winds last), the question is solved calculating the annual excedance probability _Pe_ with next formula 


```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Sorted Winds by Magnitud - wind simulation database", size="footnotesize", fig.width= 4, fig.height= 3}
par(pin = c(3, 1))
plot(1, type="n", xlab="Event Index", ylab="Wind Speed", 
     xlim=c(0,200), ylim= c(0,50), xaxt ="n", yaxt="n", bty="n", asp = 0.8)
x1 = seq(from=10, to=100, by=10)
y1 = c(45, 40, 30, 25, 20, 19, 15, 14, 13, 12)
text(x=x1,  par("usr")[3], 
     labels = x1/10, srt = 0, pos = 1, xpd = TRUE, cex=0.8)
for (i in 1:10) lines(x=c(x1[i],x1[i]), y=c(0,y1[i]))
lines(x=c(125,125), y=c(0,9))
text(x=125,  par("usr")[3], 
     labels = "..." , srt = 0, pos = 1, xpd = TRUE)
x1 = seq(from=150, to=200, by=10)
y1 = c(8, 7, 6, 5, 4, 3)
for (i in 1:10) lines(x=c(x1[i],x1[i]), y=c(0,y1[i]))
text(x=x1,  par("usr")[3], 
     labels = "." , srt = 0, pos = 1, xpd = TRUE, cex=0.8)
axis(1, labels=FALSE, tick=TRUE, col.axis="black", lwd.ticks=0)
```

$$
P_e = \frac{Event\,index\,after\,descending\,sorting}{Years\,of\,simulations } = \frac{1}{10.000}=0.001=0.01\%
$$
because the highest wind will be the first in the sorted list. Same exercise can be done with all winds to construct the annual exedance probability curve, that in this case will represent the probability to equal or exceed different probable losses due to wind.

### Return Period - Mean Recurrence Interval

Continuing with the previous section, if the inverse of the excedance probability is taken, the return period (in years) is obtained. The return period or Mean Recurrence Interval - MRI is associated with an specific return level (wind extreme velocity). MRI is the numbers of years (N) needed to obtain 63% of change that the corresponding return level will occur at least one time in that period. The return level is expected to be exceeded on average once every N-years. The annual excedance probability of the return level corresponding to N-years of MRI, is $P_e=\frac{1}{MRI}=\frac{1}{N}$.

For an specific wind extreme event A, the probability that the event will occur in a period equal to MRI years is 63%. If we analyse for the same period a strongest wind extreme event B, its orrurence probability will be lest than 67%. If the purpose of this researh is to design infrastructure considering wind loads, the structure will be more resistant to wind if we desing with stronger winds, this is high MRIs, and low annual excedance probability. Common approach for infrastructure design, considering any type of load (earthquake, wind, etc) is to choose high MRI according to the importance/use/risk/type of the structure. For highly important structures lik hospitals or coliseums, where the risk of collapse must be diminished, the MRI used to design is higher in comparison to common structures (for instance a normal house), which implies less risks for its use and importance.

$$
  \mathrm{
    P_e = 
    \begin{cases}
      \begin{split}
            &1-\exp\left(-\frac{1}{MRI}\right),\;for\,MRI\,<\,10\,years
            \\
            &\frac{1}{MRI},\;for\,MRI\,\geq\,10\,years      
      \end{split}
    \end{cases}
  }
$$

### Compound Excedance Probability - Pn

If time of exposure is consider, understood as time the structure will be in use, it is possible to have a compound probability $P_n$, where $n$ is the exposure period. $P_n$ is the probability that the extreme wind speed will be equaled or exceeded at least one time in $n$ years, and is related with the occurrence probability, but also is posible to calculate the non-ocurrence compound probability (probability that the event will not occur).

$$
  \mathrm{
    P_n = 
    \begin{cases}
      \begin{split}
            &1-\left(1-\frac{1}{MRI}\right)^n,\;occurrence\,probability
            \\
            &\left(1-\frac{1}{MRI}\right)^n,\;non-occurrence\,probability
      \end{split}
    \end{cases}
  }
$$

```{r compoundprobability, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Compound Probability", size="footnotesize", fig.width= 5, fig.height= 4}
plot(1, type="n", xlab=expression(paste("Compound Probability ", P[n])), ylab="Exposure Time as a Multiple of MRI", 
     xlim=c(0,1), ylim= c(0,2500), xaxt ="n", yaxt="n", bty="n", cex.lab=0.7)

y1 = c(0, 500,1000,1500,2000,2500)
text(y=y1,  x=par("usr")[1], 
     labels = y1/500, srt = 0, pos = 2, xpd = TRUE, cex=0.8)
y1 = 500*0.69
text(y=y1,  x=par("usr")[1], 
     labels = ".69", srt = 0, pos = 2, xpd = TRUE, cex=0.6)
y1 = 2250
text(y=y1,  x=par("usr")[1], 
     labels = expression(paste("4",frac(1,2))), srt = 0, pos = 2, xpd = TRUE, cex=0.6)

npn <- function(x) (1-(1/500))^x #Event will not occur
n = seq(from=0, to=2500, by=1)
mynpn = npn(n)

lines(x=mynpn,y=n, col= "blue")

pn <- function(x) 1-(1-(1/500))^x #Event will occur
mypn= pn(n)
lines(x=mypn,y=n, col= "green")

text(x=c(0.01, 0.37,0.63, 0.99),  par("usr")[3], 
     labels = c(".01",".37",".63",".99") , srt = 0, pos = 1, xpd = TRUE, cex=0.6)

axis(1, at= seq(from=0, to=1, by= 0.1), labels=seq(from=0, to=1, by= 0.1), tick=TRUE, col.axis="black", cex=0.8)

axis(2, at=c(0, 500,1000,1500,2000,2500),labels=FALSE, tick=TRUE, col.axis="black")
axis(2, at=c(345, 2250),labels=FALSE, tick=TRUE, col.axis="black", tck=-0.015)
axis(1, at=c(0.01,.37,.63,0.99),labels=FALSE, tick=TRUE, col.axis="black", tck=-0.015)

abline(v=c(0.01, 0.37,0.5, 0.63,0.99), lty="dotted")
abline(h=c(345,500, 2250), lty="dotted")
text(x=0.15,  y=1800, 
     labels = "chance event\nwill not occur", cex=0.7)
text(x=0.85,  y=1800, 
     labels = "chance event\nwill occur", cex=0.7)
```

If it is consider exposure time as a multiple of return period, the resulting figure  \@ref(fig:compoundprobability), shows that:

* When exposure time is .69% of the return period, then probability (ocurrence and non-occurrence) will be 50%
* As was stated previously, whe exposure time is equal to return period, then the probability that the extreme wind speed (return level) occur is 63%, and 37% for the non occurrence probability.
* If exposure time is 4.5 times the return period, there is a 99% of change that the return level will occur.

The example discused here was presented as an instrument to introduce important concepts, nonetheless,there are specialized approaches to deal with extreme value analysis which will be discussed in [Extreme Valua Analysis Overview](#extremeoverview) and more in detail in [Peaks Over Threshold - Poisson Process](#pot-pp). In summary, is neccesary to fit the data over a specific threshold to an extreme value distribution, and $P_e$ wil be $1-F(y)$, with F(y) as the _cdf_, and MRI as $\frac{1}{1-F(y)}$.


## Extreme Value Analysis Overview {#extremeoverview}

Analysis of extreme values is related with statistical inference to calculate probabilities of extreme events. Main methods to analize extreme data are ephochal, Peaks Over Threshold - POT, and extreme index. The epochal method, also known as block maxima, uses the most extreme value for a specific frame of time, tipically, one year. POT is based in the selection of a single threshold value to do the analysis only with values above the threshold. But there are different POT aproaches, the most commond one is Generalized Paretto Distribution - POT-GPD, but also it is possible to use the Poisson process approach.

In both methods (Epochal and POT), the first step is to fit the data to an appropiate probability distribution model, among them the most used are, - Extreme Value Type I (Gumbel), Extreme Value Type II (Frechet), Weibull, Generalized Pareto - GPD, and Generalized Extreme Value - GEV.

Distribution models are fitted based in the estimation of its parameters, mommonly called location, scale and shape, nonetheless each model has its own parameters names. There are different methods to estimate parameters, among them, - method of moments (modified moments - see @Kubler1994, and L moments - see @Hosking1997), - method of maximum likelihood MLE, see @Harris1994, which is problematic for GPD and GEV, - probability plot correlation coeficient, and - elemental percentiles (for GPD and GEV)

Once cadidate parameters are available, it is neccesary to assess the goodness of fit of the selected model, using one of the next methods, - Kolmogorov-Smirnov (KS) goodnes of fit test, and - Anderson-Darling goodness of fit test. Here a visual assesment is also useful using a probability plot or a kernel density plot with  the fitted _pdf_ overlaid.

The main use of the fitted model is the estimation of mean return intervals - MRI, and extreme wind speeds (return levels),

$$
MRI=\frac{1}{1-F(y)}
$$
with $F(y)$ as the _cdf_. If $1-F(y)$ is the annual excedance probability, MRI is its inverse, see @Simiu1996 for more details about MRI. If $y$ is solved from previos equation using a given MRI of N-years, its value represents the $Y_N$ wind speed return level,

$$
Y_N = G\left(1-\frac{1}{\lambda\,N}\right)
$$
where $G$ is the _ppf_ (quantile function) and $\lambda$ is the number of wind spees over the threshold per year.

The CRAN Task View "Extreme Value Analysis" <https://cran.r-project.org/web/views/ExtremeValue.html> shows available **R** for block maxima, POT by GPD, and external indexes estimation aproaches. Most important to consider are `evd`, `extremes`, `evir`, `POT`, `extremeStat`, `ismev`, and `Renext`.

 
## Peaks Over Threshold - Poisson Process {#pot-pp}

According to @Pintar2015 the stochastic poisson process is mainly defined by its intensity function. As the intensity function is nos uniform over the domain, the poisson process considered here is non-homogeneous, and due to the intensity function dependance of magnitud and time, it is also bi-dimmensional. Poisson Process was described for the first time in @Pickands1971, then extended in @Smith1989.

\begin{equation}
  \mathrm{
    \lambda\left(y,t\right)
    \begin{cases}
      \begin{split}
            &\lambda_t(y),\;for\,t\,in\,thunderstorm\,period
            \\
            &\lambda_nt(y),\;for\,t\,in\,non-thunderstorm\,period      
      \end{split}
    \end{cases}
  }
  (\#eq:ppgenericintensityfunction)
\end{equation}

Generic equation \@ref(eq:ppgenericintensityfunction) shows the intensity function, which is defined in the domain $D = D_t\,{\cup}\,D_{nt}$, and allow to fit the poisson process at each station to the observed data $\{t_i, y_i\}_{i=1}^I$ for al the times ($t_i$) of threshold crossing observations and its corresponding wind speeds magnitudes ($y_i$). Thus, only data above the threshold is used.

Intensity function of the Poisson Process is defined in @Smith2004, 

$$
\frac{1}{\psi_t}\left(1+\zeta_t\frac{y-\omega_t}{\psi_t}\right)_+^{-\frac{1}{\zeta_t}-1}
$$
Where $\zeta_t$ controls the tail lengh of the intensity function at a given time $t$, but to facilitate the estimation of the parameters then $\zeta_t$ is taken to be zero, then doing the limit, the resulting intensity function is the same as the the GEV type I or Gumbel distribution,

$$
\frac{1}{\psi_t}\exp\left\{\frac{-(y-\omega_t)}{\psi_t}\right\}
$$

In this study, the used intensity functions are shown in ecuation \@ref(eq:ppspecificintensityfunction).

\begin{equation}
  \mathrm{
    \lambda\left(y,t\right)
    \begin{cases}
      \begin{split}
            &\frac{1}{\psi_s}\exp\left\{\frac{-(y-\omega_s)}{\psi_s}\right\},\;for\,t\,in\,thunderstorm\,period
            \\
            &\frac{1}{\psi_{nt}}\exp\left\{\frac{-(y-\omega_{nt})}{\psi_{nt}}\right\},\;for\,t\,in\,non-thunderstorm\,period      
      \end{split}
    \end{cases}
  }
  (\#eq:ppspecificintensityfunction)
\end{equation}


```{r plotdomainpp, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Domanin off the Poisson Process", size="footnotesize", fig.width= 4, fig.height= 4}
plot(1, type="n", xlab="Time", ylab="Wind Speed", 
     xlim=c(0,30), ylim= c(0,30), xaxt ="n", yaxt="n", bty="n")
xpos=c(10,15,20,30)
xtick<-c(c(expression(t[1]), expression(t[2]), expression(t[3]), expression(t[4])))
text(x=xpos,  par("usr")[3], 
     labels = xtick, srt = 0, pos = 1, xpd = TRUE)

ypos=c(3,6,10,30)
ytick<-c(expression(b[nt]), expression(b[t]), expression(y[1]), expression(y[2]))
text(y=ypos,  x=par("usr")[1], 
     labels = ytick, srt = 0, pos = 2, xpd = TRUE)

rect(10,10,30,30, density = 5, col= NA, border="black")


lines(x=c(10,10), y=c(0,10), lty="dashed", col = "gray")
lines(x=c(30,30), y=c(0,10), lty="dashed", col = "gray")
lines(x=c(0,10), y=c(10,10), lty="dashed", col = "gray")
lines(x=c(0,10), y=c(30,30), lty="dashed", col = "gray")
lines(x=c(15,15), y=c(10,30), col = "black")
lines(x=c(15,15), y=c(0,10),  lty="dashed", col = "gray")

lines(x=c(20,20), y=c(10,30),  col = "black")
lines(x=c(20,20), y=c(0,10), lty="dashed", col = "gray")

lines(x=c(10,15), y=c(3,3), col = "black")
lines(x=c(20,30), y=c(3,3), col = "black")
lines(x=c(0,10), y=c(3,3), lty="dashed", col = "gray")
lines(x=c(15,20), y=c(3,3), lty="dashed", col = "gray")

lines(x=c(15,20), y=c(6,6), col = "black")
lines(x=c(20,30), y=c(6,6),  lty="dashed", col = "gray")
lines(x=c(0,15), y=c(6,6), lty="dashed", col = "gray")

rect(15,10,20,30, density = 5, angle=-45, col= NA, border="black")
#x <- "\xD0"
#Encoding(x) <- "latin1"
text(28, 28, "D", cex=1.2)
axis(1, labels=FALSE, tick=TRUE, col.axis="black", lwd.ticks=0)
axis(2, labels=FALSE, tick=TRUE, col.axis="black", lwd.ticks=0)
```

Figure \@ref(fig:plotdomainpp) represent the domain $D$ of the Poisson process. In time, the domain represents the station service period from first sample $t_1$ to last sample $t_4$. $D$ is the union of all thunderstorm periods $D_t$ (from $t_2$ to $t_3$), and all non-thunderstorm periods $D_{nt}$ (periods $t_1$ to $t_2$ and $t_3$ to $t_4$). In magnitud, only thunderstorm data above its threshold $b_t$, and only non-tunderstorm data above its threshold $b_{nt}$ are used. 

Thunderstoms and non-thunderstorms are modeled independently: 

1. Observations in domain $D$ follow a Poisson distribution with mean $\int_D\lambda(t,y)\,dt\,dy$
2. For each disjoint subdomain $D_1$ or $D_2$ inside $D$, the observations in $D_1$ or $D_2$ are independent random variables.

Visual representation of the intensity function for the Poisson Process can be seen in figure \@ref(fig:plotdomain3dpp). In vertical axis, two surfaces were drawn representing independent intensity functions for thunderstorm $\lambda_t(y)$ and for non-thunderstorm $\lambda_{nt}(y)$. The volume under each surface for its corresponding time periods and peak (over threshold) velocities, is the mean of the Poisson Process.


```{r plotdomain3dpp, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Volume under surfaces represents the mean of the Poisson process", size="footnotesize", fig.width= 5, fig.height= 5}
include_graphics(path = "figure/domain3d.PNG")
```


The method of maximun likelihood es used to estimate the parameters of the Poisson process, the selected vector of parameters $\eta$ are the $\hat\eta$ values that maximizes the function

\begin{equation}
  \mathrm{
          L(\eta)=\left( 
                        \prod_{i=1}^I\lambda\left(y_i,t_i\right)  
                  \right)
                  \exp\left\{
                            -\int_{{D}}\lambda\left(y,t\right)dy\,dt
                     \right\}
         }
  (\#eq:pplikelihood)
\end{equation}

$\hat\eta$ values need to be calculated using a numericall approach because there is not analytical solution available.

Once the Poisson process is fittet to the data, the model will provide extreme wind velocities (return levels), for different return periods (mean recurrence intervals). 


A $Y_N$ extreme wind velocity, called the return level (RL) belonging to the N-years return period, has a expected frequency to occur or to be exceeded (annual excedance probability)$P_e = \frac{1}{N}$, and also has a probability that the event does not occur (annual non-excedance probability) $P_{ne}=1-\frac{1}{N}$. $Y_N$ will be the resulting value of the $G$ (ppf or quantile) function using a probability equal to $P_{ne}$. $Y_N=quantile(y, p=P_{ne})=G(x,p=P_{ne})=ppf(x,p=P_{ne})$. As for this study $\zeta = 0$, the $G$ function to use is the Gumbel quantile function. $Y_N$ can be undestood as the wind extreme value expected to be exceeded on average once every N years.

For different POT approaches, as POT-GPD described --, the value of the probability passed to the $G$ function, has to be modified with the $\lambda$ parameter, as is described in next equation. $\lambda$ is the number of wind speed over the threshold per year.

$$
Y_N =G\left(y, 1-\frac{1}{\lambda\,N}\right)
$$

 

For the Poisson process $Y_N$ is also the solution to the next equation, which is defined in terms of the intensity function,


\begin{equation}
  \mathrm{
          \int_{Y_N}^{\infty}\int_0^1\lambda\left( y,t\right)dydt = A_t\int_{Y_N}^{\infty}\lambda_t\left( y\right)\,dy + A_{nt}\int_{Y_N}^{\infty}\lambda_{nt}\left( y\right)\,dy = \frac{1}{N}
         }
  (\#eq:ppmri)
\end{equation}

where $A_t$, is the multiplication of the average number of thunderstorm per year and the average lengh of a thunderstorm (taken to be 1 hour as defined in @Pintar2015), and $A_{nt} = 1 - A_t$. The average length of a non-thunderstorm event is variable, and it is adjusted in each station to guarantee that $A_{nt} + A_t = 1$

The same thunderstorm event in considered to occur if the time lag distance between sucesive thunderstorm samples is small than six hours, and for non-thunderstorm this time is 4 days. For the Poisson process, all the measurements belonging to the same event (thunderstorm or non tunderstorm), need to be declustered to leave only one maximun value. In other words, the number of thunderstorm in the time serie is the number of time lag distances grather than 6 hours, and for non-thunderstorm grather than 4 days.

###Threshold Selection

$$U=F(Y)$$
$$W = -log(1-U)$$

## Wind Loads Requirements

Colombian norm that defines wind loads today is the Earth 


As the output maps of this research will be used as input loads for infrastructure design, the methodology used for its creation, need to be consistent with Colombian official wind loads requirements. Today (2020), the Colombian norm that defines wind loads is the Seismic Resistant Standard 2010 - NSR-10 by its acronym of Spanish, see XXX. Chapter related to wind loads is B.6. NSR-10 was created and is maintained by the Colombian Association of Seismic Engineering - AIS. 


NSR-10 is mainly based in the USA norm American Society of Civil Engineers 7-16, minimum design loads and associated criteria for buildings and other structures - ASCE7-16, see @Asce2017. Under these circumstances, ASCE7-16 defines the minimum requirements of the research products. Especially the chapter C26 - "wind loads - general requirements", C26.5 "wind hazard map", and C26.7 "Exposure" - pages 733 to 747. Wind speeds requirements of ASCE7-16 are based in the combination of independent non-hurricane analysis, and hurricane wind speeds simulations models. The focus of this research will be the analysis of non-hurricane wind data, however, existing results of hurricane studies will be used to present final maps with both components. In ASCE7-16, for non-hurricane wind speed, the procedure is mainly based on @Pintar2015.

 
ASCE7-16 (page 734), requires the calculation of wind extreme return levels for specific return periods according to the risk category of the structure to be designed: risk category I - 300 years,  risk category II - 700 years,  risk category III - 1700 years,  risk category IV - 3000 years (NSR-10 only requires 700, 1700 and 3000 years). In addition, extreme wind speeds for those MRI need to correspond to: - 3 second gust speeds, - at 33 ft (10 meters) above the ground, and - exposure category C (open space). 

 
To standarize wind speeds to gust speeds ASCE7-16 proposes the curve Durst (see @Durst1960, and figure \@ref(fig:durstcurve)). It is valid only for open terran conditions. Durst curve shows in axis $y$ the gust factor $\frac{V_t}{V_{3600}}$, a ration between any wind gust averaged at $t$ seconds, $V_t$, and the hourly averaged wind speed $V_{3600}$, and in the axis $x$ the duration $t$ of the gust.


```{r durstcurve, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Maximun speeds averaged over t (sec), to hourly mean speed", size="footnotesize", fig.width= 5, fig.height= 5}
include_graphics(path = "figure/durst.PNG")
```


